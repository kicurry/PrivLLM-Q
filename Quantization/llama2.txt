[2026-01-05 13:26:25 root](check_overflow.py 307): INFO args: Namespace(quant_model_path='./pre_quantized_models/Llama-2-7b-hf-w4a4q4s8kv4', output_dir='./log/test', real_quant=False, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='', eval_batch_size=16, max_memory='70GiB')
[2026-01-05 13:26:47 root](check_overflow.py 342): INFO init weight quantizer
[2026-01-05 13:26:47 root](check_overflow.py 347): INFO init input quantizer
[2026-01-05 13:26:47 root](check_overflow.py 352): INFO init v quantizer
[2026-01-05 13:26:47 root](check_overflow.py 358): INFO init k quantizer
q and k-cache quantization: set model.layers.0.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.1.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.2.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.3.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.4.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.5.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.6.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.7.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.8.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.9.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.10.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.11.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.12.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.13.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.14.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.15.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.16.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.17.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.18.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.19.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.20.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.21.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.22.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.23.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.24.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.25.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.26.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.27.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.28.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.29.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.30.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.31.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
[2026-01-05 13:26:47 root](check_overflow.py 362): INFO init s quantizer
Loading pre-computed quantized weights...
[2026-01-05 13:27:48 root](check_overflow.py 370): INFO LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): QuantLinear(
            (weight_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=torch.Size([4096, 4096])
              group_size=4096
              quant_type='weight'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(4096, 1)
              zero_point=None
            )
          )
          (k_proj): QuantLinear(
            (weight_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=torch.Size([4096, 4096])
              group_size=4096
              quant_type='weight'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(4096, 1)
              zero_point=None
            )
          )
          (v_proj): QuantLinear(
            (weight_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=torch.Size([4096, 4096])
              group_size=4096
              quant_type='weight'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(4096, 1)
              zero_point=None
            )
            (output_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=(1, 4096)
              group_size=128
              quant_type='activation'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(32, 1)
              zero_point=None
            )
          )
          (o_proj): QuantLinear(
            (weight_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=torch.Size([4096, 4096])
              group_size=4096
              quant_type='weight'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(4096, 1)
              zero_point=None
            )
            (input_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=(1, 4096)
              group_size=4096
              quant_type='activation'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(1, 1)
              zero_point=None
            )
          )
          (rotary_emb): LlamaRotaryEmbedding()
          (apply_rotary_pos_emb_qk_rotation_wrapper): QKRotationWrapper(
            (k_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=(1, 4096)
              group_size=128
              quant_type='activation'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(32, 1)
              zero_point=None
            )
            (q_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=(1, 4096)
              group_size=128
              quant_type='activation'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(32, 1)
              zero_point=None
            )
          )
          (s_quantizer): UniformAffineQuantizer(
            n_bits=8
            quantized_shape=(1, 1)
            group_size=1
            quant_type='activation'
            mode='static'
            asym=False
            disable_zero_point_in_sym=True
            activation_clipping=False
            enable=True
            is_s=True
            qmin=-128, qmax=127
            scale_shape=(32, 1, 1)
            zero_point=None
          )
        )
        (mlp): LlamaMLP(
          (gate_proj): QuantLinear(
            (weight_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=torch.Size([11008, 4096])
              group_size=4096
              quant_type='weight'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(11008, 1)
              zero_point=None
            )
          )
          (up_proj): QuantLinear(
            (weight_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=torch.Size([11008, 4096])
              group_size=4096
              quant_type='weight'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(11008, 1)
              zero_point=None
            )
          )
          (down_proj): QuantLinear(
            (weight_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=torch.Size([4096, 11008])
              group_size=11008
              quant_type='weight'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(4096, 1)
              zero_point=None
            )
            (input_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=(1, 11008)
              group_size=11008
              quant_type='activation'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(1, 1)
              zero_point=None
            )
          )
          (act_fn): SiLU()
        )
        (input_layernorm): QuantRMSNorm(
          (output_quantizer): UniformAffineQuantizer(
            n_bits=4
            quantized_shape=(1, 4096)
            group_size=4096
            quant_type='activation'
            mode='static'
            asym=False
            disable_zero_point_in_sym=True
            activation_clipping=False
            enable=True
            is_s=False
            qmin=-8, qmax=7
            scale_shape=(1, 1)
            zero_point=None
          )
        )
        (post_attention_layernorm): QuantRMSNorm(
          (output_quantizer): UniformAffineQuantizer(
            n_bits=4
            quantized_shape=(1, 4096)
            group_size=4096
            quant_type='activation'
            mode='static'
            asym=False
            disable_zero_point_in_sym=True
            activation_clipping=False
            enable=True
            is_s=False
            qmin=-8, qmax=7
            scale_shape=(1, 1)
            zero_point=None
          )
        )
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
name: model.layers.0.self_attn.v_proj.output_quantizer
name: model.layers.0.self_attn.o_proj.input_quantizer
name: model.layers.0.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.0.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.0.self_attn.s_quantizer
name: model.layers.0.mlp.down_proj.input_quantizer
name: model.layers.0.input_layernorm.output_quantizer
name: model.layers.0.post_attention_layernorm.output_quantizer
name: model.layers.1.self_attn.v_proj.output_quantizer
name: model.layers.1.self_attn.o_proj.input_quantizer
name: model.layers.1.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.1.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.1.self_attn.s_quantizer
name: model.layers.1.mlp.down_proj.input_quantizer
name: model.layers.1.input_layernorm.output_quantizer
name: model.layers.1.post_attention_layernorm.output_quantizer
name: model.layers.2.self_attn.v_proj.output_quantizer
name: model.layers.2.self_attn.o_proj.input_quantizer
name: model.layers.2.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.2.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.2.self_attn.s_quantizer
name: model.layers.2.mlp.down_proj.input_quantizer
name: model.layers.2.input_layernorm.output_quantizer
name: model.layers.2.post_attention_layernorm.output_quantizer
name: model.layers.3.self_attn.v_proj.output_quantizer
name: model.layers.3.self_attn.o_proj.input_quantizer
name: model.layers.3.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.3.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.3.self_attn.s_quantizer
name: model.layers.3.mlp.down_proj.input_quantizer
name: model.layers.3.input_layernorm.output_quantizer
name: model.layers.3.post_attention_layernorm.output_quantizer
name: model.layers.4.self_attn.v_proj.output_quantizer
name: model.layers.4.self_attn.o_proj.input_quantizer
name: model.layers.4.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.4.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.4.self_attn.s_quantizer
name: model.layers.4.mlp.down_proj.input_quantizer
name: model.layers.4.input_layernorm.output_quantizer
name: model.layers.4.post_attention_layernorm.output_quantizer
name: model.layers.5.self_attn.v_proj.output_quantizer
name: model.layers.5.self_attn.o_proj.input_quantizer
name: model.layers.5.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.5.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.5.self_attn.s_quantizer
name: model.layers.5.mlp.down_proj.input_quantizer
name: model.layers.5.input_layernorm.output_quantizer
name: model.layers.5.post_attention_layernorm.output_quantizer
name: model.layers.6.self_attn.v_proj.output_quantizer
name: model.layers.6.self_attn.o_proj.input_quantizer
name: model.layers.6.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.6.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.6.self_attn.s_quantizer
name: model.layers.6.mlp.down_proj.input_quantizer
name: model.layers.6.input_layernorm.output_quantizer
name: model.layers.6.post_attention_layernorm.output_quantizer
name: model.layers.7.self_attn.v_proj.output_quantizer
name: model.layers.7.self_attn.o_proj.input_quantizer
name: model.layers.7.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.7.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.7.self_attn.s_quantizer
name: model.layers.7.mlp.down_proj.input_quantizer
name: model.layers.7.input_layernorm.output_quantizer
name: model.layers.7.post_attention_layernorm.output_quantizer
name: model.layers.8.self_attn.v_proj.output_quantizer
name: model.layers.8.self_attn.o_proj.input_quantizer
name: model.layers.8.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.8.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.8.self_attn.s_quantizer
name: model.layers.8.mlp.down_proj.input_quantizer
name: model.layers.8.input_layernorm.output_quantizer
name: model.layers.8.post_attention_layernorm.output_quantizer
name: model.layers.9.self_attn.v_proj.output_quantizer
name: model.layers.9.self_attn.o_proj.input_quantizer
name: model.layers.9.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.9.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.9.self_attn.s_quantizer
name: model.layers.9.mlp.down_proj.input_quantizer
name: model.layers.9.input_layernorm.output_quantizer
name: model.layers.9.post_attention_layernorm.output_quantizer
name: model.layers.10.self_attn.v_proj.output_quantizer
name: model.layers.10.self_attn.o_proj.input_quantizer
name: model.layers.10.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.10.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.10.self_attn.s_quantizer
name: model.layers.10.mlp.down_proj.input_quantizer
name: model.layers.10.input_layernorm.output_quantizer
name: model.layers.10.post_attention_layernorm.output_quantizer
name: model.layers.11.self_attn.v_proj.output_quantizer
name: model.layers.11.self_attn.o_proj.input_quantizer
name: model.layers.11.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.11.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.11.self_attn.s_quantizer
name: model.layers.11.mlp.down_proj.input_quantizer
name: model.layers.11.input_layernorm.output_quantizer
name: model.layers.11.post_attention_layernorm.output_quantizer
name: model.layers.12.self_attn.v_proj.output_quantizer
name: model.layers.12.self_attn.o_proj.input_quantizer
name: model.layers.12.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.12.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.12.self_attn.s_quantizer
name: model.layers.12.mlp.down_proj.input_quantizer
name: model.layers.12.input_layernorm.output_quantizer
name: model.layers.12.post_attention_layernorm.output_quantizer
name: model.layers.13.self_attn.v_proj.output_quantizer
name: model.layers.13.self_attn.o_proj.input_quantizer
name: model.layers.13.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.13.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.13.self_attn.s_quantizer
name: model.layers.13.mlp.down_proj.input_quantizer
name: model.layers.13.input_layernorm.output_quantizer
name: model.layers.13.post_attention_layernorm.output_quantizer
name: model.layers.14.self_attn.v_proj.output_quantizer
name: model.layers.14.self_attn.o_proj.input_quantizer
name: model.layers.14.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.14.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.14.self_attn.s_quantizer
name: model.layers.14.mlp.down_proj.input_quantizer
name: model.layers.14.input_layernorm.output_quantizer
name: model.layers.14.post_attention_layernorm.output_quantizer
name: model.layers.15.self_attn.v_proj.output_quantizer
name: model.layers.15.self_attn.o_proj.input_quantizer
name: model.layers.15.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.15.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.15.self_attn.s_quantizer
name: model.layers.15.mlp.down_proj.input_quantizer
name: model.layers.15.input_layernorm.output_quantizer
name: model.layers.15.post_attention_layernorm.output_quantizer
name: model.layers.16.self_attn.v_proj.output_quantizer
name: model.layers.16.self_attn.o_proj.input_quantizer
name: model.layers.16.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.16.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.16.self_attn.s_quantizer
name: model.layers.16.mlp.down_proj.input_quantizer
name: model.layers.16.input_layernorm.output_quantizer
name: model.layers.16.post_attention_layernorm.output_quantizer
name: model.layers.17.self_attn.v_proj.output_quantizer
name: model.layers.17.self_attn.o_proj.input_quantizer
name: model.layers.17.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.17.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.17.self_attn.s_quantizer
name: model.layers.17.mlp.down_proj.input_quantizer
name: model.layers.17.input_layernorm.output_quantizer
name: model.layers.17.post_attention_layernorm.output_quantizer
name: model.layers.18.self_attn.v_proj.output_quantizer
name: model.layers.18.self_attn.o_proj.input_quantizer
name: model.layers.18.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.18.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.18.self_attn.s_quantizer
name: model.layers.18.mlp.down_proj.input_quantizer
name: model.layers.18.input_layernorm.output_quantizer
name: model.layers.18.post_attention_layernorm.output_quantizer
name: model.layers.19.self_attn.v_proj.output_quantizer
name: model.layers.19.self_attn.o_proj.input_quantizer
name: model.layers.19.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.19.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.19.self_attn.s_quantizer
name: model.layers.19.mlp.down_proj.input_quantizer
name: model.layers.19.input_layernorm.output_quantizer
name: model.layers.19.post_attention_layernorm.output_quantizer
name: model.layers.20.self_attn.v_proj.output_quantizer
name: model.layers.20.self_attn.o_proj.input_quantizer
name: model.layers.20.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.20.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.20.self_attn.s_quantizer
name: model.layers.20.mlp.down_proj.input_quantizer
name: model.layers.20.input_layernorm.output_quantizer
name: model.layers.20.post_attention_layernorm.output_quantizer
name: model.layers.21.self_attn.v_proj.output_quantizer
name: model.layers.21.self_attn.o_proj.input_quantizer
name: model.layers.21.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.21.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.21.self_attn.s_quantizer
name: model.layers.21.mlp.down_proj.input_quantizer
name: model.layers.21.input_layernorm.output_quantizer
name: model.layers.21.post_attention_layernorm.output_quantizer
name: model.layers.22.self_attn.v_proj.output_quantizer
name: model.layers.22.self_attn.o_proj.input_quantizer
name: model.layers.22.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.22.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.22.self_attn.s_quantizer
name: model.layers.22.mlp.down_proj.input_quantizer
name: model.layers.22.input_layernorm.output_quantizer
name: model.layers.22.post_attention_layernorm.output_quantizer
name: model.layers.23.self_attn.v_proj.output_quantizer
name: model.layers.23.self_attn.o_proj.input_quantizer
name: model.layers.23.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.23.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.23.self_attn.s_quantizer
name: model.layers.23.mlp.down_proj.input_quantizer
name: model.layers.23.input_layernorm.output_quantizer
name: model.layers.23.post_attention_layernorm.output_quantizer
name: model.layers.24.self_attn.v_proj.output_quantizer
name: model.layers.24.self_attn.o_proj.input_quantizer
name: model.layers.24.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.24.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.24.self_attn.s_quantizer
name: model.layers.24.mlp.down_proj.input_quantizer
name: model.layers.24.input_layernorm.output_quantizer
name: model.layers.24.post_attention_layernorm.output_quantizer
name: model.layers.25.self_attn.v_proj.output_quantizer
name: model.layers.25.self_attn.o_proj.input_quantizer
name: model.layers.25.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.25.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.25.self_attn.s_quantizer
name: model.layers.25.mlp.down_proj.input_quantizer
name: model.layers.25.input_layernorm.output_quantizer
name: model.layers.25.post_attention_layernorm.output_quantizer
name: model.layers.26.self_attn.v_proj.output_quantizer
name: model.layers.26.self_attn.o_proj.input_quantizer
name: model.layers.26.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.26.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.26.self_attn.s_quantizer
name: model.layers.26.mlp.down_proj.input_quantizer
name: model.layers.26.input_layernorm.output_quantizer
name: model.layers.26.post_attention_layernorm.output_quantizer
name: model.layers.27.self_attn.v_proj.output_quantizer
name: model.layers.27.self_attn.o_proj.input_quantizer
name: model.layers.27.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.27.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.27.self_attn.s_quantizer
name: model.layers.27.mlp.down_proj.input_quantizer
name: model.layers.27.input_layernorm.output_quantizer
name: model.layers.27.post_attention_layernorm.output_quantizer
name: model.layers.28.self_attn.v_proj.output_quantizer
name: model.layers.28.self_attn.o_proj.input_quantizer
name: model.layers.28.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.28.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.28.self_attn.s_quantizer
name: model.layers.28.mlp.down_proj.input_quantizer
name: model.layers.28.input_layernorm.output_quantizer
name: model.layers.28.post_attention_layernorm.output_quantizer
name: model.layers.29.self_attn.v_proj.output_quantizer
name: model.layers.29.self_attn.o_proj.input_quantizer
name: model.layers.29.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.29.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.29.self_attn.s_quantizer
name: model.layers.29.mlp.down_proj.input_quantizer
name: model.layers.29.input_layernorm.output_quantizer
name: model.layers.29.post_attention_layernorm.output_quantizer
name: model.layers.30.self_attn.v_proj.output_quantizer
name: model.layers.30.self_attn.o_proj.input_quantizer
name: model.layers.30.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.30.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.30.self_attn.s_quantizer
name: model.layers.30.mlp.down_proj.input_quantizer
name: model.layers.30.input_layernorm.output_quantizer
name: model.layers.30.post_attention_layernorm.output_quantizer
name: model.layers.31.self_attn.v_proj.output_quantizer
name: model.layers.31.self_attn.o_proj.input_quantizer
name: model.layers.31.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.31.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.31.self_attn.s_quantizer
name: model.layers.31.mlp.down_proj.input_quantizer
name: model.layers.31.input_layernorm.output_quantizer
name: model.layers.31.post_attention_layernorm.output_quantizer
[{'qkv_var': tensor(4.9180, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0027, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.2188, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0063, device='cuda:0', dtype=torch.float16), 'k_var': tensor(5.2266, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0025, device='cuda:0', dtype=torch.float16), 'q_var': tensor(2.4375, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-1.6153e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.4160, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1948, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.6074, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0062, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.0234, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0112, device='cuda:0', dtype=torch.float16), 'down_var': tensor(7.6133, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0314, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(6.2109, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0040, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.2305, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0097, device='cuda:0', dtype=torch.float16), 'k_var': tensor(7.8906, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0062, device='cuda:0', dtype=torch.float16), 'q_var': tensor(3.0859, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-0.0003, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.0508, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.2072, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.2441, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(0.0042, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(9.3906, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0237, device='cuda:0', dtype=torch.float16), 'down_var': tensor(7.9805, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0087, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.3281, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0159, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.6172, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0195, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.3750, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0079, device='cuda:0', dtype=torch.float16), 'q_var': tensor(3.2246, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(7.9274e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(6.7188, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.2325, device='cuda:0', dtype=torch.float16), 'o_var': tensor(3.4141, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0266, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.4844, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0250, device='cuda:0', dtype=torch.float16), 'down_var': tensor(5.1094, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(0.0099, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.0508, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0206, device='cuda:0', dtype=torch.float16), 'v_var': tensor(8., device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0149, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.2344, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0097, device='cuda:0', dtype=torch.float16), 'q_var': tensor(3.9727, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-6.8069e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.6055, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0969, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.9590, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(0.0087, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.4336, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0169, device='cuda:0', dtype=torch.float16), 'down_var': tensor(7.6133, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0044, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.1094, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0174, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.6172, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0205, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.3281, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0096, device='cuda:0', dtype=torch.float16), 'q_var': tensor(3.8066, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(2.3246e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.5791, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0866, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.5391, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0100, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.5234, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0155, device='cuda:0', dtype=torch.float16), 'down_var': tensor(7.2539, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0022, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.3906, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0151, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.7031, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(0.0089, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.1641, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0095, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(2.3425e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.4180, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0704, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.3828, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0110, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(9.4609, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0155, device='cuda:0', dtype=torch.float16), 'down_var': tensor(5.7734, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0020, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.4688, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0129, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.7305, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0002, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.3828, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0095, device='cuda:0', dtype=torch.float16), 'q_var': tensor(3.9824, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-6.2227e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.6123, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1009, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.2031, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0103, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.2461, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0063, device='cuda:0', dtype=torch.float16), 'down_var': tensor(8.5547, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0122, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(6.8359, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0019, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.8672, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0066, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.4766, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0095, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4., device='cuda:0', dtype=torch.float16), 'q_mean': tensor(3.2902e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.2441, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1011, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.4492, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0060, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(9.1875, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0053, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.3516, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(0.0027, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(6.5352, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0027, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.2266, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(0.0047, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.3672, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0102, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.0039, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-4.2200e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.8818, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1044, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.9180, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0106, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(9.2812, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0076, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.4648, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0008, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(6.4453, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0014, device='cuda:0', dtype=torch.float16), 'v_var': tensor(4.9688, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0094, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.3281, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0095, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(9.1195e-06, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.8066, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0885, device='cuda:0', dtype=torch.float16), 'o_var': tensor(3.2227, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0158, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(8.8281, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0035, device='cuda:0', dtype=torch.float16), 'down_var': tensor(8.0234, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0051, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(6.9492, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0046, device='cuda:0', dtype=torch.float16), 'v_var': tensor(3.0273, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(0.0036, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.7734, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0086, device='cuda:0', dtype=torch.float16), 'q_var': tensor(3.8828, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-4.9233e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.2461, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1134, device='cuda:0', dtype=torch.float16), 'o_var': tensor(4.0078, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0102, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.4570, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0062, device='cuda:0', dtype=torch.float16), 'down_var': tensor(7.3359, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0023, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.2266, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0048, device='cuda:0', dtype=torch.float16), 'v_var': tensor(4.8867, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0036, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.2500, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0092, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-0.0001, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.0391, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1207, device='cuda:0', dtype=torch.float16), 'o_var': tensor(3.7500, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0184, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.5234, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0109, device='cuda:0', dtype=torch.float16), 'down_var': tensor(7.3711, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0045, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.4961, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0059, device='cuda:0', dtype=torch.float16), 'v_var': tensor(4.1328, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0076, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.0703, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0095, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(3.3915e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.0508, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1068, device='cuda:0', dtype=torch.float16), 'o_var': tensor(3.0254, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0072, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.4922, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0023, device='cuda:0', dtype=torch.float16), 'down_var': tensor(7.9492, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0076, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.5156, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0012, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.0547, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0047, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.7656, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0088, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-6.2346e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.8047, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1252, device='cuda:0', dtype=torch.float16), 'o_var': tensor(3.5898, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0142, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.7070, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0007, device='cuda:0', dtype=torch.float16), 'down_var': tensor(7.6172, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0024, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.6992, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0019, device='cuda:0', dtype=torch.float16), 'v_var': tensor(5.2734, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(0.0039, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.7656, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0080, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-0.0001, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.9043, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1007, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.6016, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(0.0011, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.7031, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0007, device='cuda:0', dtype=torch.float16), 'down_var': tensor(8.0078, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0047, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.3672, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0015, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.0547, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0083, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.7656, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0087, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-1.9193e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.8105, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0889, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.6348, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0080, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.1992, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0031, device='cuda:0', dtype=torch.float16), 'down_var': tensor(8.0156, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(0.0040, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.3359, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0044, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.0742, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0016, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.7656, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0082, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-3.1769e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.5117, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0701, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.2441, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0053, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.2266, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0045, device='cuda:0', dtype=torch.float16), 'down_var': tensor(7.2148, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0032, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.7031, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0103, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.8359, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0046, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.0547, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0088, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-0.0002, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.2207, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0582, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.0488, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0113, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.5117, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0068, device='cuda:0', dtype=torch.float16), 'down_var': tensor(7.6250, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0035, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.5938, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0100, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.8320, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0070, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.2266, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0081, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-9.5367e-06, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.4160, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0594, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.3867, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0072, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.7344, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0094, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.8477, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0076, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.5117, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0100, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.9844, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0063, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.4141, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0095, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.3711, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-5.8055e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.3174, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0656, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.3213, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(0.0016, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.5234, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0125, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.3242, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0072, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.5547, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0112, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.8359, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0203, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.5391, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0103, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.3008, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-6.5506e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.1230, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0566, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.9346, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0082, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.8828, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0137, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.0586, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0042, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.6523, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0121, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.8867, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0109, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.0078, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0081, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1484, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-0.0002, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.3857, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0594, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.1094, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0023, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.8125, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0110, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.5625, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0068, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.8906, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0102, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.2344, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0116, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.1797, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0087, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.2539, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-0.0001, device='cuda:0', dtype=torch.float16), 's_var': tensor(0.9429, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0412, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.3271, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0051, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.5742, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0086, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.7617, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(0.0003, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.8008, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0081, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.5547, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0104, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.5391, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0095, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-0.0001, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.0645, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0417, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.2246, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(0.0027, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.8438, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0087, device='cuda:0', dtype=torch.float16), 'down_var': tensor(5.5664, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(0.0004, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(8.1250, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0052, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.3047, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0097, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.5391, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0096, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-4.1008e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(0.8047, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0442, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.3135, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0097, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.6133, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0029, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.5391, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0055, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.2930, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0020, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.6562, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(0.0055, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.8672, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0103, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-8.2254e-06, device='cuda:0', dtype=torch.float16), 's_var': tensor(0.8413, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0430, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.8320, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0170, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.2891, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0023, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.5742, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0037, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(8.1406, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0047, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.2734, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0015, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.9688, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0111, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-0.0002, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.0352, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0507, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.8545, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0093, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.8906, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0018, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.4570, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0091, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.7656, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0007, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.6133, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0155, device='cuda:0', dtype=torch.float16), 'k_var': tensor(11.0156, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0098, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(1.7047e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(0.8101, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0350, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.4150, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0037, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.4922, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0007, device='cuda:0', dtype=torch.float16), 'down_var': tensor(8.6172, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0161, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.5703, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0006, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.6133, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0221, device='cuda:0', dtype=torch.float16), 'k_var': tensor(11.3281, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0114, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.0859, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-6.6102e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(0.8770, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0415, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.4648, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0117, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.5000, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(6.5744e-05, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.4453, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0086, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.6289, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0014, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.6133, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(0.0130, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.4141, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0087, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.2891, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-9.1136e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.0244, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0565, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.3984, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(0.0137, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.9570, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0029, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.8320, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0042, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(8.2031, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0041, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.6133, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0327, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.5391, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0088, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.2930, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-9.9778e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(0.8813, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0383, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.1709, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0086, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.7891, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0009, device='cuda:0', dtype=torch.float16), 'down_var': tensor(7.6211, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(0.0012, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(9.4375, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0132, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.0781, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0073, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.7734, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0089, device='cuda:0', dtype=torch.float16), 'q_var': tensor(3.6133, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-9.1791e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.5908, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0715, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.1221, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0046, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.2500, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0059, device='cuda:0', dtype=torch.float16), 'down_var': tensor(5.6719, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(0.0054, device='cuda:0', dtype=torch.float16)}]
[{'model.layers.0.self_attn.q_proj': 0.0, 'model.layers.0.self_attn.q_proj_mean': array(-0.002708, dtype=float16), 'model.layers.0.self_attn.q_proj_sigma': array(2.217, dtype=float16), 'model.layers.0.self_attn.k_proj': 0.0, 'model.layers.0.self_attn.k_proj_mean': array(-0.002708, dtype=float16), 'model.layers.0.self_attn.k_proj_sigma': array(2.217, dtype=float16), 'model.layers.0.self_attn.v_proj': 0.0, 'model.layers.0.self_attn.v_proj_mean': array(-0.002708, dtype=float16), 'model.layers.0.self_attn.v_proj_sigma': array(2.217, dtype=float16), 'model.layers.0.self_attn.o_proj': 0.0, 'model.layers.0.self_attn.o_proj_mean': array(-0.006176, dtype=float16), 'model.layers.0.self_attn.o_proj_sigma': array(1.268, dtype=float16), 'model.layers.0.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.0.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 5.1897368393838406e-06, 'model.layers.0.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 40.39801975344831, 'model.layers.0.self_attn.s_quantizer': 0.0, 'model.layers.0.self_attn.s_quantizer_mean': -0.156296968460083, 'model.layers.0.self_attn.s_quantizer_sigma': 34.044089061098404, 'model.layers.0.mlp.gate_proj': 0.0, 'model.layers.0.mlp.gate_proj_mean': array(-0.01121, dtype=float16), 'model.layers.0.mlp.gate_proj_sigma': array(2.65, dtype=float16), 'model.layers.0.mlp.up_proj': 0.0, 'model.layers.0.mlp.up_proj_mean': array(-0.01121, dtype=float16), 'model.layers.0.mlp.up_proj_sigma': array(2.65, dtype=float16), 'model.layers.0.mlp.down_proj': 0.0, 'model.layers.0.mlp.down_proj_mean': array(-0.0314, dtype=float16), 'model.layers.0.mlp.down_proj_sigma': array(2.76, dtype=float16)}, {'model.layers.1.self_attn.q_proj': 0.0, 'model.layers.1.self_attn.q_proj_mean': array(-0.00404, dtype=float16), 'model.layers.1.self_attn.q_proj_sigma': array(2.492, dtype=float16), 'model.layers.1.self_attn.k_proj': 0.0, 'model.layers.1.self_attn.k_proj_mean': array(-0.00404, dtype=float16), 'model.layers.1.self_attn.k_proj_sigma': array(2.492, dtype=float16), 'model.layers.1.self_attn.v_proj': 0.0, 'model.layers.1.self_attn.v_proj_mean': array(-0.00404, dtype=float16), 'model.layers.1.self_attn.v_proj_sigma': array(2.492, dtype=float16), 'model.layers.1.self_attn.o_proj': 0.0, 'model.layers.1.self_attn.o_proj_mean': array(0.00424, dtype=float16), 'model.layers.1.self_attn.o_proj_sigma': array(1.498, dtype=float16), 'model.layers.1.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.1.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.0002324976958334446, 'model.layers.1.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 55.82114294781145, 'model.layers.1.self_attn.s_quantizer': 0.0, 'model.layers.1.self_attn.s_quantizer_mean': -0.25772786140441895, 'model.layers.1.self_attn.s_quantizer_sigma': 44.01136216933077, 'model.layers.1.mlp.gate_proj': 0.0, 'model.layers.1.mlp.gate_proj_mean': array(-0.02365, dtype=float16), 'model.layers.1.mlp.gate_proj_sigma': array(3.064, dtype=float16), 'model.layers.1.mlp.up_proj': 0.0, 'model.layers.1.mlp.up_proj_mean': array(-0.02365, dtype=float16), 'model.layers.1.mlp.up_proj_sigma': array(3.064, dtype=float16), 'model.layers.1.mlp.down_proj': 0.0, 'model.layers.1.mlp.down_proj_mean': array(-0.008675, dtype=float16), 'model.layers.1.mlp.down_proj_sigma': array(2.824, dtype=float16)}, {'model.layers.2.self_attn.q_proj': 0.0, 'model.layers.2.self_attn.q_proj_mean': array(-0.01587, dtype=float16), 'model.layers.2.self_attn.q_proj_sigma': array(2.707, dtype=float16), 'model.layers.2.self_attn.k_proj': 0.0, 'model.layers.2.self_attn.k_proj_mean': array(-0.01587, dtype=float16), 'model.layers.2.self_attn.k_proj_sigma': array(2.707, dtype=float16), 'model.layers.2.self_attn.v_proj': 0.0, 'model.layers.2.self_attn.v_proj_mean': array(-0.01587, dtype=float16), 'model.layers.2.self_attn.v_proj_sigma': array(2.707, dtype=float16), 'model.layers.2.self_attn.o_proj': 0.0, 'model.layers.2.self_attn.o_proj_mean': array(-0.02664, dtype=float16), 'model.layers.2.self_attn.o_proj_sigma': array(1.848, dtype=float16), 'model.layers.2.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.2.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -8.020317181944847e-05, 'model.layers.2.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 62.20932405998316, 'model.layers.2.self_attn.s_quantizer': 0.0, 'model.layers.2.self_attn.s_quantizer_mean': -0.5795431137084961, 'model.layers.2.self_attn.s_quantizer_sigma': 81.26499861564018, 'model.layers.2.mlp.gate_proj': 0.0, 'model.layers.2.mlp.gate_proj_mean': array(-0.02504, dtype=float16), 'model.layers.2.mlp.gate_proj_sigma': array(2.736, dtype=float16), 'model.layers.2.mlp.up_proj': 0.0, 'model.layers.2.mlp.up_proj_mean': array(-0.02504, dtype=float16), 'model.layers.2.mlp.up_proj_sigma': array(2.736, dtype=float16), 'model.layers.2.mlp.down_proj': 0.0, 'model.layers.2.mlp.down_proj_mean': array(0.00993, dtype=float16), 'model.layers.2.mlp.down_proj_sigma': array(2.26, dtype=float16)}, {'model.layers.3.self_attn.q_proj': 0.0, 'model.layers.3.self_attn.q_proj_mean': array(-0.02058, dtype=float16), 'model.layers.3.self_attn.q_proj_sigma': array(2.656, dtype=float16), 'model.layers.3.self_attn.k_proj': 0.0, 'model.layers.3.self_attn.k_proj_mean': array(-0.02058, dtype=float16), 'model.layers.3.self_attn.k_proj_sigma': array(2.656, dtype=float16), 'model.layers.3.self_attn.v_proj': 0.0, 'model.layers.3.self_attn.v_proj_mean': array(-0.02058, dtype=float16), 'model.layers.3.self_attn.v_proj_sigma': array(2.656, dtype=float16), 'model.layers.3.self_attn.o_proj': 0.0, 'model.layers.3.self_attn.o_proj_mean': array(0.0087, dtype=float16), 'model.layers.3.self_attn.o_proj_sigma': array(1.399, dtype=float16), 'model.layers.3.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.3.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 8.488621097058058e-05, 'model.layers.3.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 72.13875518748573, 'model.layers.3.self_attn.s_quantizer': 0.0, 'model.layers.3.self_attn.s_quantizer_mean': -0.1844770908355713, 'model.layers.3.self_attn.s_quantizer_sigma': 40.65710270051225, 'model.layers.3.mlp.gate_proj': 0.0, 'model.layers.3.mlp.gate_proj_mean': array(-0.01694, dtype=float16), 'model.layers.3.mlp.gate_proj_sigma': array(2.727, dtype=float16), 'model.layers.3.mlp.up_proj': 0.0, 'model.layers.3.mlp.up_proj_mean': array(-0.01694, dtype=float16), 'model.layers.3.mlp.up_proj_sigma': array(2.727, dtype=float16), 'model.layers.3.mlp.down_proj': 0.0, 'model.layers.3.mlp.down_proj_mean': array(-0.004436, dtype=float16), 'model.layers.3.mlp.down_proj_sigma': array(2.76, dtype=float16)}, {'model.layers.4.self_attn.q_proj': 0.0, 'model.layers.4.self_attn.q_proj_mean': array(-0.0174, dtype=float16), 'model.layers.4.self_attn.q_proj_sigma': array(2.666, dtype=float16), 'model.layers.4.self_attn.k_proj': 0.0, 'model.layers.4.self_attn.k_proj_mean': array(-0.0174, dtype=float16), 'model.layers.4.self_attn.k_proj_sigma': array(2.666, dtype=float16), 'model.layers.4.self_attn.v_proj': 0.0, 'model.layers.4.self_attn.v_proj_mean': array(-0.0174, dtype=float16), 'model.layers.4.self_attn.v_proj_sigma': array(2.666, dtype=float16), 'model.layers.4.self_attn.o_proj': 0.0, 'model.layers.4.self_attn.o_proj_mean': array(-0.010025, dtype=float16), 'model.layers.4.self_attn.o_proj_sigma': array(1.24, dtype=float16), 'model.layers.4.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.4.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -2.8535141609609127e-05, 'model.layers.4.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 70.90839160494335, 'model.layers.4.self_attn.s_quantizer': 0.0, 'model.layers.4.self_attn.s_quantizer_mean': -0.22684085369110107, 'model.layers.4.self_attn.s_quantizer_sigma': 39.344631145812, 'model.layers.4.mlp.gate_proj': 0.0, 'model.layers.4.mlp.gate_proj_mean': array(-0.01547, dtype=float16), 'model.layers.4.mlp.gate_proj_sigma': array(2.742, dtype=float16), 'model.layers.4.mlp.up_proj': 0.0, 'model.layers.4.mlp.up_proj_mean': array(-0.01547, dtype=float16), 'model.layers.4.mlp.up_proj_sigma': array(2.742, dtype=float16), 'model.layers.4.mlp.down_proj': 0.0, 'model.layers.4.mlp.down_proj_mean': array(-0.002213, dtype=float16), 'model.layers.4.mlp.down_proj_sigma': array(2.693, dtype=float16)}, {'model.layers.5.self_attn.q_proj': 0.0, 'model.layers.5.self_attn.q_proj_mean': array(-0.01512, dtype=float16), 'model.layers.5.self_attn.q_proj_sigma': array(2.719, dtype=float16), 'model.layers.5.self_attn.k_proj': 0.0, 'model.layers.5.self_attn.k_proj_mean': array(-0.01512, dtype=float16), 'model.layers.5.self_attn.k_proj_sigma': array(2.719, dtype=float16), 'model.layers.5.self_attn.v_proj': 0.0, 'model.layers.5.self_attn.v_proj_mean': array(-0.01512, dtype=float16), 'model.layers.5.self_attn.v_proj_sigma': array(2.719, dtype=float16), 'model.layers.5.self_attn.o_proj': 0.0, 'model.layers.5.self_attn.o_proj_mean': array(-0.010994, dtype=float16), 'model.layers.5.self_attn.o_proj_sigma': array(1.544, dtype=float16), 'model.layers.5.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.5.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -2.857163781300187e-05, 'model.layers.5.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 73.02054505411473, 'model.layers.5.self_attn.s_quantizer': 0.0, 'model.layers.5.self_attn.s_quantizer_mean': 0.07972002029418945, 'model.layers.5.self_attn.s_quantizer_sigma': 37.44329045369811, 'model.layers.5.mlp.gate_proj': 0.0, 'model.layers.5.mlp.gate_proj_mean': array(-0.0155, dtype=float16), 'model.layers.5.mlp.gate_proj_sigma': array(3.076, dtype=float16), 'model.layers.5.mlp.up_proj': 0.0, 'model.layers.5.mlp.up_proj_mean': array(-0.0155, dtype=float16), 'model.layers.5.mlp.up_proj_sigma': array(3.076, dtype=float16), 'model.layers.5.mlp.down_proj': 0.0, 'model.layers.5.mlp.down_proj_mean': array(-0.002022, dtype=float16), 'model.layers.5.mlp.down_proj_sigma': array(2.402, dtype=float16)}, {'model.layers.6.self_attn.q_proj': 0.0, 'model.layers.6.self_attn.q_proj_mean': array(-0.01287, dtype=float16), 'model.layers.6.self_attn.q_proj_sigma': array(2.732, dtype=float16), 'model.layers.6.self_attn.k_proj': 0.0, 'model.layers.6.self_attn.k_proj_mean': array(-0.01287, dtype=float16), 'model.layers.6.self_attn.k_proj_sigma': array(2.732, dtype=float16), 'model.layers.6.self_attn.v_proj': 0.0, 'model.layers.6.self_attn.v_proj_mean': array(-0.01287, dtype=float16), 'model.layers.6.self_attn.v_proj_sigma': array(2.732, dtype=float16), 'model.layers.6.self_attn.o_proj': 0.0, 'model.layers.6.self_attn.o_proj_mean': array(-0.0103, dtype=float16), 'model.layers.6.self_attn.o_proj_sigma': array(1.484, dtype=float16), 'model.layers.6.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.6.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 7.577869109809399e-05, 'model.layers.6.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 72.74613391789285, 'model.layers.6.self_attn.s_quantizer': 0.0, 'model.layers.6.self_attn.s_quantizer_mean': -0.002667912282049656, 'model.layers.6.self_attn.s_quantizer_sigma': 40.06245124802026, 'model.layers.6.mlp.gate_proj': 0.0, 'model.layers.6.mlp.gate_proj_mean': array(0.00626, dtype=float16), 'model.layers.6.mlp.gate_proj_sigma': array(2.691, dtype=float16), 'model.layers.6.mlp.up_proj': 0.0, 'model.layers.6.mlp.up_proj_mean': array(0.00626, dtype=float16), 'model.layers.6.mlp.up_proj_sigma': array(2.691, dtype=float16), 'model.layers.6.mlp.down_proj': 0.0, 'model.layers.6.mlp.down_proj_mean': array(-0.01218, dtype=float16), 'model.layers.6.mlp.down_proj_sigma': array(2.926, dtype=float16)}, {'model.layers.7.self_attn.q_proj': 0.0, 'model.layers.7.self_attn.q_proj_mean': array(-0.001942, dtype=float16), 'model.layers.7.self_attn.q_proj_sigma': array(2.615, dtype=float16), 'model.layers.7.self_attn.k_proj': 0.0, 'model.layers.7.self_attn.k_proj_mean': array(-0.001942, dtype=float16), 'model.layers.7.self_attn.k_proj_sigma': array(2.615, dtype=float16), 'model.layers.7.self_attn.v_proj': 0.0, 'model.layers.7.self_attn.v_proj_mean': array(-0.001942, dtype=float16), 'model.layers.7.self_attn.v_proj_sigma': array(2.615, dtype=float16), 'model.layers.7.self_attn.o_proj': 0.0, 'model.layers.7.self_attn.o_proj_mean': array(-0.00596, dtype=float16), 'model.layers.7.self_attn.o_proj_sigma': array(1.565, dtype=float16), 'model.layers.7.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.7.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -4.000263288617134e-05, 'model.layers.7.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 73.23933369440222, 'model.layers.7.self_attn.s_quantizer': 0.0, 'model.layers.7.self_attn.s_quantizer_mean': -0.08557748794555664, 'model.layers.7.self_attn.s_quantizer_sigma': 47.644516998286385, 'model.layers.7.mlp.gate_proj': 0.0, 'model.layers.7.mlp.gate_proj_mean': array(-0.005314, dtype=float16), 'model.layers.7.mlp.gate_proj_sigma': array(3.031, dtype=float16), 'model.layers.7.mlp.up_proj': 0.0, 'model.layers.7.mlp.up_proj_mean': array(-0.005314, dtype=float16), 'model.layers.7.mlp.up_proj_sigma': array(3.031, dtype=float16), 'model.layers.7.mlp.down_proj': 0.0, 'model.layers.7.mlp.down_proj_mean': array(0.002659, dtype=float16), 'model.layers.7.mlp.down_proj_sigma': array(2.52, dtype=float16)}, {'model.layers.8.self_attn.q_proj': 0.0, 'model.layers.8.self_attn.q_proj_mean': array(-0.002659, dtype=float16), 'model.layers.8.self_attn.q_proj_sigma': array(2.557, dtype=float16), 'model.layers.8.self_attn.k_proj': 0.0, 'model.layers.8.self_attn.k_proj_mean': array(-0.002659, dtype=float16), 'model.layers.8.self_attn.k_proj_sigma': array(2.557, dtype=float16), 'model.layers.8.self_attn.v_proj': 0.0, 'model.layers.8.self_attn.v_proj_mean': array(-0.002659, dtype=float16), 'model.layers.8.self_attn.v_proj_sigma': array(2.557, dtype=float16), 'model.layers.8.self_attn.o_proj': 0.0, 'model.layers.8.self_attn.o_proj_mean': array(-0.01059, dtype=float16), 'model.layers.8.self_attn.o_proj_sigma': array(1.708, dtype=float16), 'model.layers.8.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.8.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 5.514035001397133e-05, 'model.layers.8.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 72.8834686331544, 'model.layers.8.self_attn.s_quantizer': 0.0, 'model.layers.8.self_attn.s_quantizer_mean': 0.062479376792907715, 'model.layers.8.self_attn.s_quantizer_sigma': 41.84495190581536, 'model.layers.8.mlp.gate_proj': 0.0, 'model.layers.8.mlp.gate_proj_mean': array(-0.007553, dtype=float16), 'model.layers.8.mlp.gate_proj_sigma': array(3.047, dtype=float16), 'model.layers.8.mlp.up_proj': 0.0, 'model.layers.8.mlp.up_proj_mean': array(-0.007553, dtype=float16), 'model.layers.8.mlp.up_proj_sigma': array(3.047, dtype=float16), 'model.layers.8.mlp.down_proj': 0.0, 'model.layers.8.mlp.down_proj_mean': array(-0.000816, dtype=float16), 'model.layers.8.mlp.down_proj_sigma': array(2.543, dtype=float16)}, {'model.layers.9.self_attn.q_proj': 0.0, 'model.layers.9.self_attn.q_proj_mean': array(0.001422, dtype=float16), 'model.layers.9.self_attn.q_proj_sigma': array(2.54, dtype=float16), 'model.layers.9.self_attn.k_proj': 0.0, 'model.layers.9.self_attn.k_proj_mean': array(0.001422, dtype=float16), 'model.layers.9.self_attn.k_proj_sigma': array(2.54, dtype=float16), 'model.layers.9.self_attn.v_proj': 0.0, 'model.layers.9.self_attn.v_proj_mean': array(0.001422, dtype=float16), 'model.layers.9.self_attn.v_proj_sigma': array(2.54, dtype=float16), 'model.layers.9.self_attn.o_proj': 0.0, 'model.layers.9.self_attn.o_proj_mean': array(-0.0158, dtype=float16), 'model.layers.9.self_attn.o_proj_sigma': array(1.795, dtype=float16), 'model.layers.9.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.9.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -1.1132215149700642e-05, 'model.layers.9.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 73.62064927722385, 'model.layers.9.self_attn.s_quantizer': 0.0, 'model.layers.9.self_attn.s_quantizer_mean': -0.10604560375213623, 'model.layers.9.self_attn.s_quantizer_sigma': 33.94112549695428, 'model.layers.9.mlp.gate_proj': 0.0, 'model.layers.9.mlp.gate_proj_mean': array(-0.003548, dtype=float16), 'model.layers.9.mlp.gate_proj_sigma': array(2.97, dtype=float16), 'model.layers.9.mlp.up_proj': 0.0, 'model.layers.9.mlp.up_proj_mean': array(-0.003548, dtype=float16), 'model.layers.9.mlp.up_proj_sigma': array(2.97, dtype=float16), 'model.layers.9.mlp.down_proj': 0.0, 'model.layers.9.mlp.down_proj_mean': array(-0.005123, dtype=float16), 'model.layers.9.mlp.down_proj_sigma': array(2.832, dtype=float16)}, {'model.layers.10.self_attn.q_proj': 0.0, 'model.layers.10.self_attn.q_proj_mean': array(0.004593, dtype=float16), 'model.layers.10.self_attn.q_proj_sigma': array(2.637, dtype=float16), 'model.layers.10.self_attn.k_proj': 0.0, 'model.layers.10.self_attn.k_proj_mean': array(0.004593, dtype=float16), 'model.layers.10.self_attn.k_proj_sigma': array(2.637, dtype=float16), 'model.layers.10.self_attn.v_proj': 0.0, 'model.layers.10.self_attn.v_proj_mean': array(0.004593, dtype=float16), 'model.layers.10.self_attn.v_proj_sigma': array(2.637, dtype=float16), 'model.layers.10.self_attn.o_proj': 0.0, 'model.layers.10.self_attn.o_proj_mean': array(-0.01023, dtype=float16), 'model.layers.10.self_attn.o_proj_sigma': array(2.002, dtype=float16), 'model.layers.10.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.10.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 5.413754843175411e-05, 'model.layers.10.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 69.7137002317335, 'model.layers.10.self_attn.s_quantizer': 0.0, 'model.layers.10.self_attn.s_quantizer_mean': 0.05279788374900818, 'model.layers.10.self_attn.s_quantizer_sigma': 29.605742686174924, 'model.layers.10.mlp.gate_proj': 0.0, 'model.layers.10.mlp.gate_proj_mean': array(0.006245, dtype=float16), 'model.layers.10.mlp.gate_proj_sigma': array(2.73, dtype=float16), 'model.layers.10.mlp.up_proj': 0.0, 'model.layers.10.mlp.up_proj_mean': array(0.006245, dtype=float16), 'model.layers.10.mlp.up_proj_sigma': array(2.73, dtype=float16), 'model.layers.10.mlp.down_proj': 0.0, 'model.layers.10.mlp.down_proj_mean': array(-0.00232, dtype=float16), 'model.layers.10.mlp.down_proj_sigma': array(2.709, dtype=float16)}, {'model.layers.11.self_attn.q_proj': 0.0, 'model.layers.11.self_attn.q_proj_mean': array(0.004757, dtype=float16), 'model.layers.11.self_attn.q_proj_sigma': array(2.688, dtype=float16), 'model.layers.11.self_attn.k_proj': 0.0, 'model.layers.11.self_attn.k_proj_mean': array(0.004757, dtype=float16), 'model.layers.11.self_attn.k_proj_sigma': array(2.688, dtype=float16), 'model.layers.11.self_attn.v_proj': 0.0, 'model.layers.11.self_attn.v_proj_mean': array(0.004757, dtype=float16), 'model.layers.11.self_attn.v_proj_sigma': array(2.688, dtype=float16), 'model.layers.11.self_attn.o_proj': 0.0, 'model.layers.11.self_attn.o_proj_mean': array(-0.01839, dtype=float16), 'model.layers.11.self_attn.o_proj_sigma': array(1.937, dtype=float16), 'model.layers.11.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.11.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.0001722518354654312, 'model.layers.11.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 73.348483283569, 'model.layers.11.self_attn.s_quantizer': 0.0, 'model.layers.11.self_attn.s_quantizer_mean': -0.05588485300540924, 'model.layers.11.self_attn.s_quantizer_sigma': 35.832945734337834, 'model.layers.11.mlp.gate_proj': 0.0, 'model.layers.11.mlp.gate_proj_mean': array(0.01094, dtype=float16), 'model.layers.11.mlp.gate_proj_sigma': array(2.742, dtype=float16), 'model.layers.11.mlp.up_proj': 0.0, 'model.layers.11.mlp.up_proj_mean': array(0.01094, dtype=float16), 'model.layers.11.mlp.up_proj_sigma': array(2.742, dtype=float16), 'model.layers.11.mlp.down_proj': 0.0, 'model.layers.11.mlp.down_proj_mean': array(-0.00451, dtype=float16), 'model.layers.11.mlp.down_proj_sigma': array(2.715, dtype=float16)}, {'model.layers.12.self_attn.q_proj': 0.0, 'model.layers.12.self_attn.q_proj_mean': array(0.005924, dtype=float16), 'model.layers.12.self_attn.q_proj_sigma': array(2.738, dtype=float16), 'model.layers.12.self_attn.k_proj': 0.0, 'model.layers.12.self_attn.k_proj_mean': array(0.005924, dtype=float16), 'model.layers.12.self_attn.k_proj_sigma': array(2.738, dtype=float16), 'model.layers.12.self_attn.v_proj': 0.0, 'model.layers.12.self_attn.v_proj_mean': array(0.005924, dtype=float16), 'model.layers.12.self_attn.v_proj_sigma': array(2.738, dtype=float16), 'model.layers.12.self_attn.o_proj': 0.0, 'model.layers.12.self_attn.o_proj_mean': array(-0.00717, dtype=float16), 'model.layers.12.self_attn.o_proj_sigma': array(1.739, dtype=float16), 'model.layers.12.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.12.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -4.123459802940488e-05, 'model.layers.12.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 72.71863585079137, 'model.layers.12.self_attn.s_quantizer': 0.0, 'model.layers.12.self_attn.s_quantizer_mean': -0.10419639945030212, 'model.layers.12.self_attn.s_quantizer_sigma': 33.015148038438355, 'model.layers.12.mlp.gate_proj': 0.0, 'model.layers.12.mlp.gate_proj_mean': array(0.00227, dtype=float16), 'model.layers.12.mlp.gate_proj_sigma': array(2.736, dtype=float16), 'model.layers.12.mlp.up_proj': 0.0, 'model.layers.12.mlp.up_proj_mean': array(0.00227, dtype=float16), 'model.layers.12.mlp.up_proj_sigma': array(2.736, dtype=float16), 'model.layers.12.mlp.down_proj': 0.0, 'model.layers.12.mlp.down_proj_mean': array(-0.007565, dtype=float16), 'model.layers.12.mlp.down_proj_sigma': array(2.82, dtype=float16)}, {'model.layers.13.self_attn.q_proj': 0.0, 'model.layers.13.self_attn.q_proj_mean': array(0.001192, dtype=float16), 'model.layers.13.self_attn.q_proj_sigma': array(2.742, dtype=float16), 'model.layers.13.self_attn.k_proj': 0.0, 'model.layers.13.self_attn.k_proj_mean': array(0.001192, dtype=float16), 'model.layers.13.self_attn.k_proj_sigma': array(2.742, dtype=float16), 'model.layers.13.self_attn.v_proj': 0.0, 'model.layers.13.self_attn.v_proj_mean': array(0.001192, dtype=float16), 'model.layers.13.self_attn.v_proj_sigma': array(2.742, dtype=float16), 'model.layers.13.self_attn.o_proj': 0.0, 'model.layers.13.self_attn.o_proj_mean': array(-0.01419, dtype=float16), 'model.layers.13.self_attn.o_proj_sigma': array(1.895, dtype=float16), 'model.layers.13.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.13.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 6.983533967286348e-05, 'model.layers.13.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 71.61005515987263, 'model.layers.13.self_attn.s_quantizer': 0.0, 'model.layers.13.self_attn.s_quantizer_mean': -0.07521986961364746, 'model.layers.13.self_attn.s_quantizer_sigma': 46.75467891024384, 'model.layers.13.mlp.gate_proj': 0.0, 'model.layers.13.mlp.gate_proj_mean': array(0.000727, dtype=float16), 'model.layers.13.mlp.gate_proj_sigma': array(2.775, dtype=float16), 'model.layers.13.mlp.up_proj': 0.0, 'model.layers.13.mlp.up_proj_mean': array(0.000727, dtype=float16), 'model.layers.13.mlp.up_proj_sigma': array(2.775, dtype=float16), 'model.layers.13.mlp.down_proj': 0.0, 'model.layers.13.mlp.down_proj_mean': array(-0.002445, dtype=float16), 'model.layers.13.mlp.down_proj_sigma': array(2.76, dtype=float16)}, {'model.layers.14.self_attn.q_proj': 0.0, 'model.layers.14.self_attn.q_proj_mean': array(0.001891, dtype=float16), 'model.layers.14.self_attn.q_proj_sigma': array(2.775, dtype=float16), 'model.layers.14.self_attn.k_proj': 0.0, 'model.layers.14.self_attn.k_proj_mean': array(0.001891, dtype=float16), 'model.layers.14.self_attn.k_proj_sigma': array(2.775, dtype=float16), 'model.layers.14.self_attn.v_proj': 0.0, 'model.layers.14.self_attn.v_proj_mean': array(0.001891, dtype=float16), 'model.layers.14.self_attn.v_proj_sigma': array(2.775, dtype=float16), 'model.layers.14.self_attn.o_proj': 0.0, 'model.layers.14.self_attn.o_proj_mean': array(0.001074, dtype=float16), 'model.layers.14.self_attn.o_proj_sigma': array(1.613, dtype=float16), 'model.layers.14.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.14.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.00015132827684283257, 'model.layers.14.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 71.61005515987263, 'model.layers.14.self_attn.s_quantizer': 0.0, 'model.layers.14.self_attn.s_quantizer_mean': 0.0504031777381897, 'model.layers.14.self_attn.s_quantizer_sigma': 35.958309192730404, 'model.layers.14.mlp.gate_proj': 0.0, 'model.layers.14.mlp.gate_proj_mean': array(0.000678, dtype=float16), 'model.layers.14.mlp.gate_proj_sigma': array(2.775, dtype=float16), 'model.layers.14.mlp.up_proj': 0.0, 'model.layers.14.mlp.up_proj_mean': array(0.000678, dtype=float16), 'model.layers.14.mlp.up_proj_sigma': array(2.775, dtype=float16), 'model.layers.14.mlp.down_proj': 0.0, 'model.layers.14.mlp.down_proj_mean': array(-0.004715, dtype=float16), 'model.layers.14.mlp.down_proj_sigma': array(2.83, dtype=float16)}, {'model.layers.15.self_attn.q_proj': 0.0, 'model.layers.15.self_attn.q_proj_mean': array(-0.001459, dtype=float16), 'model.layers.15.self_attn.q_proj_sigma': array(2.715, dtype=float16), 'model.layers.15.self_attn.k_proj': 0.0, 'model.layers.15.self_attn.k_proj_mean': array(-0.001459, dtype=float16), 'model.layers.15.self_attn.k_proj_sigma': array(2.715, dtype=float16), 'model.layers.15.self_attn.v_proj': 0.0, 'model.layers.15.self_attn.v_proj_mean': array(-0.001459, dtype=float16), 'model.layers.15.self_attn.v_proj_sigma': array(2.715, dtype=float16), 'model.layers.15.self_attn.o_proj': 0.0, 'model.layers.15.self_attn.o_proj_mean': array(-0.00804, dtype=float16), 'model.layers.15.self_attn.o_proj_sigma': array(1.623, dtype=float16), 'model.layers.15.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.15.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 2.140435390174389e-05, 'model.layers.15.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 71.61005515987263, 'model.layers.15.self_attn.s_quantizer': 0.0, 'model.layers.15.self_attn.s_quantizer_mean': -0.09492045640945435, 'model.layers.15.self_attn.s_quantizer_sigma': 37.53664875824692, 'model.layers.15.mlp.gate_proj': 0.0, 'model.layers.15.mlp.gate_proj_mean': array(-0.003132, dtype=float16), 'model.layers.15.mlp.gate_proj_sigma': array(2.684, dtype=float16), 'model.layers.15.mlp.up_proj': 0.0, 'model.layers.15.mlp.up_proj_mean': array(-0.003132, dtype=float16), 'model.layers.15.mlp.up_proj_sigma': array(2.684, dtype=float16), 'model.layers.15.mlp.down_proj': 0.0, 'model.layers.15.mlp.down_proj_mean': array(0.004032, dtype=float16), 'model.layers.15.mlp.down_proj_sigma': array(2.832, dtype=float16)}, {'model.layers.16.self_attn.q_proj': 0.0, 'model.layers.16.self_attn.q_proj_mean': array(-0.004406, dtype=float16), 'model.layers.16.self_attn.q_proj_sigma': array(2.709, dtype=float16), 'model.layers.16.self_attn.k_proj': 0.0, 'model.layers.16.self_attn.k_proj_mean': array(-0.004406, dtype=float16), 'model.layers.16.self_attn.k_proj_sigma': array(2.709, dtype=float16), 'model.layers.16.self_attn.v_proj': 0.0, 'model.layers.16.self_attn.v_proj_mean': array(-0.004406, dtype=float16), 'model.layers.16.self_attn.v_proj_sigma': array(2.709, dtype=float16), 'model.layers.16.self_attn.o_proj': 0.0, 'model.layers.16.self_attn.o_proj_mean': array(-0.005337, dtype=float16), 'model.layers.16.self_attn.o_proj_sigma': array(1.498, dtype=float16), 'model.layers.16.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.16.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 3.328948514536023e-05, 'model.layers.16.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 71.61005515987263, 'model.layers.16.self_attn.s_quantizer': 0.0, 'model.layers.16.self_attn.s_quantizer_mean': -0.013979651033878326, 'model.layers.16.self_attn.s_quantizer_sigma': 34.336569426778794, 'model.layers.16.mlp.gate_proj': 0.0, 'model.layers.16.mlp.gate_proj_mean': array(-0.004547, dtype=float16), 'model.layers.16.mlp.gate_proj_sigma': array(2.688, dtype=float16), 'model.layers.16.mlp.up_proj': 0.0, 'model.layers.16.mlp.up_proj_mean': array(-0.004547, dtype=float16), 'model.layers.16.mlp.up_proj_sigma': array(2.688, dtype=float16), 'model.layers.16.mlp.down_proj': 0.0, 'model.layers.16.mlp.down_proj_mean': array(-0.003222, dtype=float16), 'model.layers.16.mlp.down_proj_sigma': array(2.686, dtype=float16)}, {'model.layers.17.self_attn.q_proj': 0.0, 'model.layers.17.self_attn.q_proj_mean': array(-0.01034, dtype=float16), 'model.layers.17.self_attn.q_proj_sigma': array(2.775, dtype=float16), 'model.layers.17.self_attn.k_proj': 0.0, 'model.layers.17.self_attn.k_proj_mean': array(-0.01034, dtype=float16), 'model.layers.17.self_attn.k_proj_sigma': array(2.775, dtype=float16), 'model.layers.17.self_attn.v_proj': 0.0, 'model.layers.17.self_attn.v_proj_mean': array(-0.01034, dtype=float16), 'model.layers.17.self_attn.v_proj_sigma': array(2.775, dtype=float16), 'model.layers.17.self_attn.o_proj': 0.0, 'model.layers.17.self_attn.o_proj_mean': array(-0.01134, dtype=float16), 'model.layers.17.self_attn.o_proj_sigma': array(1.432, dtype=float16), 'model.layers.17.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.17.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.0001849367981776595, 'model.layers.17.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 72.69112738154499, 'model.layers.17.self_attn.s_quantizer': 0.0, 'model.layers.17.self_attn.s_quantizer_mean': -0.03416714072227478, 'model.layers.17.self_attn.s_quantizer_sigma': 32.72613634390714, 'model.layers.17.mlp.gate_proj': 0.0, 'model.layers.17.mlp.gate_proj_mean': array(-0.006752, dtype=float16), 'model.layers.17.mlp.gate_proj_sigma': array(2.74, dtype=float16), 'model.layers.17.mlp.up_proj': 0.0, 'model.layers.17.mlp.up_proj_mean': array(-0.006752, dtype=float16), 'model.layers.17.mlp.up_proj_sigma': array(2.74, dtype=float16), 'model.layers.17.mlp.down_proj': 0.0, 'model.layers.17.mlp.down_proj_mean': array(-0.003477, dtype=float16), 'model.layers.17.mlp.down_proj_sigma': array(2.762, dtype=float16)}, {'model.layers.18.self_attn.q_proj': 0.0, 'model.layers.18.self_attn.q_proj_mean': array(-0.009964, dtype=float16), 'model.layers.18.self_attn.q_proj_sigma': array(2.756, dtype=float16), 'model.layers.18.self_attn.k_proj': 0.0, 'model.layers.18.self_attn.k_proj_mean': array(-0.009964, dtype=float16), 'model.layers.18.self_attn.k_proj_sigma': array(2.756, dtype=float16), 'model.layers.18.self_attn.v_proj': 0.0, 'model.layers.18.self_attn.v_proj_mean': array(-0.009964, dtype=float16), 'model.layers.18.self_attn.v_proj_sigma': array(2.756, dtype=float16), 'model.layers.18.self_attn.o_proj': 0.0, 'model.layers.18.self_attn.o_proj_mean': array(-0.0072, dtype=float16), 'model.layers.18.self_attn.o_proj_sigma': array(1.545, dtype=float16), 'model.layers.18.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.18.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 9.834766387939453e-06, 'model.layers.18.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 73.23933369440222, 'model.layers.18.self_attn.s_quantizer': 0.0, 'model.layers.18.self_attn.s_quantizer_mean': -0.05288803577423096, 'model.layers.18.self_attn.s_quantizer_sigma': 35.22782990761707, 'model.layers.18.mlp.gate_proj': 0.0, 'model.layers.18.mlp.gate_proj_mean': array(-0.0094, dtype=float16), 'model.layers.18.mlp.gate_proj_sigma': array(2.781, dtype=float16), 'model.layers.18.mlp.up_proj': 0.0, 'model.layers.18.mlp.up_proj_mean': array(-0.0094, dtype=float16), 'model.layers.18.mlp.up_proj_sigma': array(2.781, dtype=float16), 'model.layers.18.mlp.down_proj': 0.0, 'model.layers.18.mlp.down_proj_mean': array(-0.007587, dtype=float16), 'model.layers.18.mlp.down_proj_sigma': array(2.617, dtype=float16)}, {'model.layers.19.self_attn.q_proj': 0.0, 'model.layers.19.self_attn.q_proj_mean': array(-0.01, dtype=float16), 'model.layers.19.self_attn.q_proj_sigma': array(2.74, dtype=float16), 'model.layers.19.self_attn.k_proj': 0.0, 'model.layers.19.self_attn.k_proj_mean': array(-0.01, dtype=float16), 'model.layers.19.self_attn.k_proj_sigma': array(2.74, dtype=float16), 'model.layers.19.self_attn.v_proj': 0.0, 'model.layers.19.self_attn.v_proj_mean': array(-0.01, dtype=float16), 'model.layers.19.self_attn.v_proj_sigma': array(2.74, dtype=float16), 'model.layers.19.self_attn.o_proj': 0.0, 'model.layers.19.self_attn.o_proj_mean': array(0.001554, dtype=float16), 'model.layers.19.self_attn.o_proj_sigma': array(1.149, dtype=float16), 'model.layers.19.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.19.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 7.024419028311968e-05, 'model.layers.19.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 76.28892449104261, 'model.layers.19.self_attn.s_quantizer': 0.0, 'model.layers.19.self_attn.s_quantizer_mean': -0.05321428179740906, 'model.layers.19.self_attn.s_quantizer_sigma': 34.36568055487916, 'model.layers.19.mlp.gate_proj': 0.0, 'model.layers.19.mlp.gate_proj_mean': array(-0.012505, dtype=float16), 'model.layers.19.mlp.gate_proj_sigma': array(2.742, dtype=float16), 'model.layers.19.mlp.up_proj': 0.0, 'model.layers.19.mlp.up_proj_mean': array(-0.012505, dtype=float16), 'model.layers.19.mlp.up_proj_sigma': array(2.742, dtype=float16), 'model.layers.19.mlp.down_proj': 0.0, 'model.layers.19.mlp.down_proj_mean': array(-0.00716, dtype=float16), 'model.layers.19.mlp.down_proj_sigma': array(2.516, dtype=float16)}, {'model.layers.20.self_attn.q_proj': 0.0, 'model.layers.20.self_attn.q_proj_mean': array(-0.01119, dtype=float16), 'model.layers.20.self_attn.q_proj_sigma': array(2.748, dtype=float16), 'model.layers.20.self_attn.k_proj': 0.0, 'model.layers.20.self_attn.k_proj_mean': array(-0.01119, dtype=float16), 'model.layers.20.self_attn.k_proj_sigma': array(2.748, dtype=float16), 'model.layers.20.self_attn.v_proj': 0.0, 'model.layers.20.self_attn.v_proj_mean': array(-0.01119, dtype=float16), 'model.layers.20.self_attn.v_proj_sigma': array(2.748, dtype=float16), 'model.layers.20.self_attn.o_proj': 0.0, 'model.layers.20.self_attn.o_proj_mean': array(-0.0082, dtype=float16), 'model.layers.20.self_attn.o_proj_sigma': array(1.391, dtype=float16), 'model.layers.20.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.20.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 8.610391523689032e-05, 'model.layers.20.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 76.15773105863909, 'model.layers.20.self_attn.s_quantizer': 0.0, 'model.layers.20.self_attn.s_quantizer_mean': -0.14727473258972168, 'model.layers.20.self_attn.s_quantizer_sigma': 31.400636936215164, 'model.layers.20.mlp.gate_proj': 0.0, 'model.layers.20.mlp.gate_proj_mean': array(-0.0137, dtype=float16), 'model.layers.20.mlp.gate_proj_sigma': array(2.809, dtype=float16), 'model.layers.20.mlp.up_proj': 0.0, 'model.layers.20.mlp.up_proj_mean': array(-0.0137, dtype=float16), 'model.layers.20.mlp.up_proj_sigma': array(2.809, dtype=float16), 'model.layers.20.mlp.down_proj': 0.0, 'model.layers.20.mlp.down_proj_mean': array(-0.004242, dtype=float16), 'model.layers.20.mlp.down_proj_sigma': array(2.46, dtype=float16)}, {'model.layers.21.self_attn.q_proj': 0.0, 'model.layers.21.self_attn.q_proj_mean': array(-0.012146, dtype=float16), 'model.layers.21.self_attn.q_proj_sigma': array(2.766, dtype=float16), 'model.layers.21.self_attn.k_proj': 0.0, 'model.layers.21.self_attn.k_proj_mean': array(-0.012146, dtype=float16), 'model.layers.21.self_attn.k_proj_sigma': array(2.766, dtype=float16), 'model.layers.21.self_attn.v_proj': 0.0, 'model.layers.21.self_attn.v_proj_mean': array(-0.012146, dtype=float16), 'model.layers.21.self_attn.v_proj_sigma': array(2.766, dtype=float16), 'model.layers.21.self_attn.o_proj': 0.0, 'model.layers.21.self_attn.o_proj_mean': array(-0.002277, dtype=float16), 'model.layers.21.self_attn.o_proj_sigma': array(1.054, dtype=float16), 'model.layers.21.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.21.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.00016134930774569511, 'model.layers.21.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 72.91090453423274, 'model.layers.21.self_attn.s_quantizer': 0.0, 'model.layers.21.self_attn.s_quantizer_mean': -0.08277475833892822, 'model.layers.21.self_attn.s_quantizer_sigma': 34.9857113690718, 'model.layers.21.mlp.gate_proj': 0.0, 'model.layers.21.mlp.gate_proj_mean': array(-0.01096, dtype=float16), 'model.layers.21.mlp.gate_proj_sigma': array(2.795, dtype=float16), 'model.layers.21.mlp.up_proj': 0.0, 'model.layers.21.mlp.up_proj_mean': array(-0.01096, dtype=float16), 'model.layers.21.mlp.up_proj_sigma': array(2.795, dtype=float16), 'model.layers.21.mlp.down_proj': 0.0, 'model.layers.21.mlp.down_proj_mean': array(-0.006844, dtype=float16), 'model.layers.21.mlp.down_proj_sigma': array(2.562, dtype=float16)}, {'model.layers.22.self_attn.q_proj': 0.0, 'model.layers.22.self_attn.q_proj_mean': array(-0.01025, dtype=float16), 'model.layers.22.self_attn.q_proj_sigma': array(2.809, dtype=float16), 'model.layers.22.self_attn.k_proj': 0.0, 'model.layers.22.self_attn.k_proj_mean': array(-0.01025, dtype=float16), 'model.layers.22.self_attn.k_proj_sigma': array(2.809, dtype=float16), 'model.layers.22.self_attn.v_proj': 0.0, 'model.layers.22.self_attn.v_proj_mean': array(-0.01025, dtype=float16), 'model.layers.22.self_attn.v_proj_sigma': array(2.809, dtype=float16), 'model.layers.22.self_attn.o_proj': 0.0, 'model.layers.22.self_attn.o_proj_mean': array(-0.005146, dtype=float16), 'model.layers.22.self_attn.o_proj_sigma': array(1.152, dtype=float16), 'model.layers.22.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.22.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.0001518245553597808, 'model.layers.22.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 74.48489779814429, 'model.layers.22.self_attn.s_quantizer': 0.0, 'model.layers.22.self_attn.s_quantizer_mean': -0.06122967600822449, 'model.layers.22.self_attn.s_quantizer_sigma': 29.58039891549808, 'model.layers.22.mlp.gate_proj': 0.0, 'model.layers.22.mlp.gate_proj_mean': array(-0.008644, dtype=float16), 'model.layers.22.mlp.gate_proj_sigma': array(2.752, dtype=float16), 'model.layers.22.mlp.up_proj': 0.0, 'model.layers.22.mlp.up_proj_mean': array(-0.008644, dtype=float16), 'model.layers.22.mlp.up_proj_sigma': array(2.752, dtype=float16), 'model.layers.22.mlp.down_proj': 0.0, 'model.layers.22.mlp.down_proj_mean': array(0.0002651, dtype=float16), 'model.layers.22.mlp.down_proj_sigma': array(2.6, dtype=float16)}, {'model.layers.23.self_attn.q_proj': 0.0, 'model.layers.23.self_attn.q_proj_mean': array(-0.008064, dtype=float16), 'model.layers.23.self_attn.q_proj_sigma': array(2.793, dtype=float16), 'model.layers.23.self_attn.k_proj': 0.0, 'model.layers.23.self_attn.k_proj_mean': array(-0.008064, dtype=float16), 'model.layers.23.self_attn.k_proj_sigma': array(2.793, dtype=float16), 'model.layers.23.self_attn.v_proj': 0.0, 'model.layers.23.self_attn.v_proj_mean': array(-0.008064, dtype=float16), 'model.layers.23.self_attn.v_proj_sigma': array(2.793, dtype=float16), 'model.layers.23.self_attn.o_proj': 0.0, 'model.layers.23.self_attn.o_proj_mean': array(0.002676, dtype=float16), 'model.layers.23.self_attn.o_proj_sigma': array(1.106, dtype=float16), 'model.layers.23.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.23.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.0001685217721387744, 'model.layers.23.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 74.3774159271482, 'model.layers.23.self_attn.s_quantizer': 0.0, 'model.layers.23.self_attn.s_quantizer_mean': -0.05548739433288574, 'model.layers.23.self_attn.s_quantizer_sigma': 32.09361307176243, 'model.layers.23.mlp.gate_proj': 0.0, 'model.layers.23.mlp.gate_proj_mean': array(-0.00866, dtype=float16), 'model.layers.23.mlp.gate_proj_sigma': array(2.8, dtype=float16), 'model.layers.23.mlp.up_proj': 0.0, 'model.layers.23.mlp.up_proj_mean': array(-0.00866, dtype=float16), 'model.layers.23.mlp.up_proj_sigma': array(2.8, dtype=float16), 'model.layers.23.mlp.down_proj': 0.0, 'model.layers.23.mlp.down_proj_mean': array(0.0004263, dtype=float16), 'model.layers.23.mlp.down_proj_sigma': array(2.36, dtype=float16)}, {'model.layers.24.self_attn.q_proj': 0.0, 'model.layers.24.self_attn.q_proj_mean': array(-0.00525, dtype=float16), 'model.layers.24.self_attn.q_proj_sigma': array(2.85, dtype=float16), 'model.layers.24.self_attn.k_proj': 0.0, 'model.layers.24.self_attn.k_proj_mean': array(-0.00525, dtype=float16), 'model.layers.24.self_attn.k_proj_sigma': array(2.85, dtype=float16), 'model.layers.24.self_attn.v_proj': 0.0, 'model.layers.24.self_attn.v_proj_mean': array(-0.00525, dtype=float16), 'model.layers.24.self_attn.v_proj_sigma': array(2.85, dtype=float16), 'model.layers.24.self_attn.o_proj': 0.0, 'model.layers.24.self_attn.o_proj_mean': array(-0.0097, dtype=float16), 'model.layers.24.self_attn.o_proj_sigma': array(1.146, dtype=float16), 'model.layers.24.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.24.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 5.041901022195816e-05, 'model.layers.24.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 74.3774159271482, 'model.layers.24.self_attn.s_quantizer': 0.0, 'model.layers.24.self_attn.s_quantizer_mean': -0.05468118190765381, 'model.layers.24.self_attn.s_quantizer_sigma': 27.477263328068172, 'model.layers.24.mlp.gate_proj': 0.0, 'model.layers.24.mlp.gate_proj_mean': array(-0.002905, dtype=float16), 'model.layers.24.mlp.gate_proj_sigma': array(2.76, dtype=float16), 'model.layers.24.mlp.up_proj': 0.0, 'model.layers.24.mlp.up_proj_mean': array(-0.002905, dtype=float16), 'model.layers.24.mlp.up_proj_sigma': array(2.76, dtype=float16), 'model.layers.24.mlp.down_proj': 0.0, 'model.layers.24.mlp.down_proj_mean': array(-0.0055, dtype=float16), 'model.layers.24.mlp.down_proj_sigma': array(2.557, dtype=float16)}, {'model.layers.25.self_attn.q_proj': 0.0, 'model.layers.25.self_attn.q_proj_mean': array(-0.001991, dtype=float16), 'model.layers.25.self_attn.q_proj_sigma': array(2.701, dtype=float16), 'model.layers.25.self_attn.k_proj': 0.0, 'model.layers.25.self_attn.k_proj_mean': array(-0.001991, dtype=float16), 'model.layers.25.self_attn.k_proj_sigma': array(2.701, dtype=float16), 'model.layers.25.self_attn.v_proj': 0.0, 'model.layers.25.self_attn.v_proj_mean': array(-0.001991, dtype=float16), 'model.layers.25.self_attn.v_proj_sigma': array(2.701, dtype=float16), 'model.layers.25.self_attn.o_proj': 0.0, 'model.layers.25.self_attn.o_proj_mean': array(-0.01698, dtype=float16), 'model.layers.25.self_attn.o_proj_sigma': array(1.683, dtype=float16), 'model.layers.25.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.25.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 1.083605457097292e-05, 'model.layers.25.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 75.52483035399682, 'model.layers.25.self_attn.s_quantizer': 0.0, 'model.layers.25.self_attn.s_quantizer_mean': 0.030254364013671875, 'model.layers.25.self_attn.s_quantizer_sigma': 28.7489130229301, 'model.layers.25.mlp.gate_proj': 0.0, 'model.layers.25.mlp.gate_proj_mean': array(-0.002348, dtype=float16), 'model.layers.25.mlp.gate_proj_sigma': array(2.7, dtype=float16), 'model.layers.25.mlp.up_proj': 0.0, 'model.layers.25.mlp.up_proj_mean': array(-0.002348, dtype=float16), 'model.layers.25.mlp.up_proj_sigma': array(2.7, dtype=float16), 'model.layers.25.mlp.down_proj': 0.0, 'model.layers.25.mlp.down_proj_mean': array(-0.003706, dtype=float16), 'model.layers.25.mlp.down_proj_sigma': array(2.564, dtype=float16)}, {'model.layers.26.self_attn.q_proj': 0.0, 'model.layers.26.self_attn.q_proj_mean': array(-0.004723, dtype=float16), 'model.layers.26.self_attn.q_proj_sigma': array(2.854, dtype=float16), 'model.layers.26.self_attn.k_proj': 0.0, 'model.layers.26.self_attn.k_proj_mean': array(-0.004723, dtype=float16), 'model.layers.26.self_attn.k_proj_sigma': array(2.854, dtype=float16), 'model.layers.26.self_attn.v_proj': 0.0, 'model.layers.26.self_attn.v_proj_mean': array(-0.004723, dtype=float16), 'model.layers.26.self_attn.v_proj_sigma': array(2.854, dtype=float16), 'model.layers.26.self_attn.o_proj': 0.0, 'model.layers.26.self_attn.o_proj_mean': array(-0.009285, dtype=float16), 'model.layers.26.self_attn.o_proj_sigma': array(1.361, dtype=float16), 'model.layers.26.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.26.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.0002375096082687378, 'model.layers.26.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 75.8946638440411, 'model.layers.26.self_attn.s_quantizer': 0.0, 'model.layers.26.self_attn.s_quantizer_mean': -0.009540997445583344, 'model.layers.26.self_attn.s_quantizer_sigma': 31.080540535840107, 'model.layers.26.mlp.gate_proj': 0.0, 'model.layers.26.mlp.gate_proj_mean': array(-0.001761, dtype=float16), 'model.layers.26.mlp.gate_proj_sigma': array(2.809, dtype=float16), 'model.layers.26.mlp.up_proj': 0.0, 'model.layers.26.mlp.up_proj_mean': array(-0.001761, dtype=float16), 'model.layers.26.mlp.up_proj_sigma': array(2.809, dtype=float16), 'model.layers.26.mlp.down_proj': 0.0, 'model.layers.26.mlp.down_proj_mean': array(-0.009094, dtype=float16), 'model.layers.26.mlp.down_proj_sigma': array(2.541, dtype=float16)}, {'model.layers.27.self_attn.q_proj': 0.0, 'model.layers.27.self_attn.q_proj_mean': array(-0.0006943, dtype=float16), 'model.layers.27.self_attn.q_proj_sigma': array(2.787, dtype=float16), 'model.layers.27.self_attn.k_proj': 0.0, 'model.layers.27.self_attn.k_proj_mean': array(-0.0006943, dtype=float16), 'model.layers.27.self_attn.k_proj_sigma': array(2.787, dtype=float16), 'model.layers.27.self_attn.v_proj': 0.0, 'model.layers.27.self_attn.v_proj_mean': array(-0.0006943, dtype=float16), 'model.layers.27.self_attn.v_proj_sigma': array(2.787, dtype=float16), 'model.layers.27.self_attn.o_proj': 0.0, 'model.layers.27.self_attn.o_proj_mean': array(-0.003658, dtype=float16), 'model.layers.27.self_attn.o_proj_sigma': array(1.189, dtype=float16), 'model.layers.27.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.27.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -2.1441839635372162e-05, 'model.layers.27.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 76.02631123499285, 'model.layers.27.self_attn.s_quantizer': 0.0, 'model.layers.27.self_attn.s_quantizer_mean': -0.06938409805297852, 'model.layers.27.self_attn.s_quantizer_sigma': 28.124722220850465, 'model.layers.27.mlp.gate_proj': 0.0, 'model.layers.27.mlp.gate_proj_mean': array(0.000711, dtype=float16), 'model.layers.27.mlp.gate_proj_sigma': array(2.736, dtype=float16), 'model.layers.27.mlp.up_proj': 0.0, 'model.layers.27.mlp.up_proj_mean': array(0.000711, dtype=float16), 'model.layers.27.mlp.up_proj_sigma': array(2.736, dtype=float16), 'model.layers.27.mlp.down_proj': 0.0, 'model.layers.27.mlp.down_proj_mean': array(-0.01611, dtype=float16), 'model.layers.27.mlp.down_proj_sigma': array(2.936, dtype=float16)}, {'model.layers.28.self_attn.q_proj': 0.0, 'model.layers.28.self_attn.q_proj_mean': array(-0.0006027, dtype=float16), 'model.layers.28.self_attn.q_proj_sigma': array(2.752, dtype=float16), 'model.layers.28.self_attn.k_proj': 0.0, 'model.layers.28.self_attn.k_proj_mean': array(-0.0006027, dtype=float16), 'model.layers.28.self_attn.k_proj_sigma': array(2.752, dtype=float16), 'model.layers.28.self_attn.v_proj': 0.0, 'model.layers.28.self_attn.v_proj_mean': array(-0.0006027, dtype=float16), 'model.layers.28.self_attn.v_proj_sigma': array(2.752, dtype=float16), 'model.layers.28.self_attn.o_proj': 0.0, 'model.layers.28.self_attn.o_proj_mean': array(-0.01175, dtype=float16), 'model.layers.28.self_attn.o_proj_sigma': array(1.21, dtype=float16), 'model.layers.28.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.28.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 9.682844392955303e-05, 'model.layers.28.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 76.96752561957543, 'model.layers.28.self_attn.s_quantizer': 0.0, 'model.layers.28.self_attn.s_quantizer_mean': -0.11721611022949219, 'model.layers.28.self_attn.s_quantizer_sigma': 29.266021253323792, 'model.layers.28.mlp.gate_proj': 0.0, 'model.layers.28.mlp.gate_proj_mean': array(6.574e-05, dtype=float16), 'model.layers.28.mlp.gate_proj_sigma': array(2.738, dtype=float16), 'model.layers.28.mlp.up_proj': 0.0, 'model.layers.28.mlp.up_proj_mean': array(6.574e-05, dtype=float16), 'model.layers.28.mlp.up_proj_sigma': array(2.738, dtype=float16), 'model.layers.28.mlp.down_proj': 0.0, 'model.layers.28.mlp.down_proj_mean': array(-0.00864, dtype=float16), 'model.layers.28.mlp.down_proj_sigma': array(2.54, dtype=float16)}, {'model.layers.29.self_attn.q_proj': 0.0, 'model.layers.29.self_attn.q_proj_mean': array(-0.001427, dtype=float16), 'model.layers.29.self_attn.q_proj_sigma': array(2.762, dtype=float16), 'model.layers.29.self_attn.k_proj': 0.0, 'model.layers.29.self_attn.k_proj_mean': array(-0.001427, dtype=float16), 'model.layers.29.self_attn.k_proj_sigma': array(2.762, dtype=float16), 'model.layers.29.self_attn.v_proj': 0.0, 'model.layers.29.self_attn.v_proj_mean': array(-0.001427, dtype=float16), 'model.layers.29.self_attn.v_proj_sigma': array(2.762, dtype=float16), 'model.layers.29.self_attn.o_proj': 0.0, 'model.layers.29.self_attn.o_proj_mean': array(0.01366, dtype=float16), 'model.layers.29.self_attn.o_proj_sigma': array(1.549, dtype=float16), 'model.layers.29.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.29.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.00010190444299951196, 'model.layers.29.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 75.57777451076474, 'model.layers.29.self_attn.s_quantizer': 0.0, 'model.layers.29.self_attn.s_quantizer_mean': 0.09383857250213623, 'model.layers.29.self_attn.s_quantizer_sigma': 31.63858403911275, 'model.layers.29.mlp.gate_proj': 0.0, 'model.layers.29.mlp.gate_proj_mean': array(-0.002855, dtype=float16), 'model.layers.29.mlp.gate_proj_sigma': array(2.82, dtype=float16), 'model.layers.29.mlp.up_proj': 0.0, 'model.layers.29.mlp.up_proj_mean': array(-0.002855, dtype=float16), 'model.layers.29.mlp.up_proj_sigma': array(2.82, dtype=float16), 'model.layers.29.mlp.down_proj': 0.0, 'model.layers.29.mlp.down_proj_mean': array(-0.00425, dtype=float16), 'model.layers.29.mlp.down_proj_sigma': array(2.613, dtype=float16)}, {'model.layers.30.self_attn.q_proj': 0.0, 'model.layers.30.self_attn.q_proj_mean': array(-0.00411, dtype=float16), 'model.layers.30.self_attn.q_proj_sigma': array(2.863, dtype=float16), 'model.layers.30.self_attn.k_proj': 0.0, 'model.layers.30.self_attn.k_proj_mean': array(-0.00411, dtype=float16), 'model.layers.30.self_attn.k_proj_sigma': array(2.863, dtype=float16), 'model.layers.30.self_attn.v_proj': 0.0, 'model.layers.30.self_attn.v_proj_mean': array(-0.00411, dtype=float16), 'model.layers.30.self_attn.v_proj_sigma': array(2.863, dtype=float16), 'model.layers.30.self_attn.o_proj': 0.0, 'model.layers.30.self_attn.o_proj_mean': array(-0.00862, dtype=float16), 'model.layers.30.self_attn.o_proj_sigma': array(1.082, dtype=float16), 'model.layers.30.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.30.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.0001120555680245161, 'model.layers.30.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 76.10519036176179, 'model.layers.30.self_attn.s_quantizer': 0.0, 'model.layers.30.self_attn.s_quantizer_mean': -0.15995264053344727, 'model.layers.30.self_attn.s_quantizer_sigma': 29.351320242878344, 'model.layers.30.mlp.gate_proj': 0.0, 'model.layers.30.mlp.gate_proj_mean': array(-0.0008826, dtype=float16), 'model.layers.30.mlp.gate_proj_sigma': array(2.791, dtype=float16), 'model.layers.30.mlp.up_proj': 0.0, 'model.layers.30.mlp.up_proj_mean': array(-0.0008826, dtype=float16), 'model.layers.30.mlp.up_proj_sigma': array(2.791, dtype=float16), 'model.layers.30.mlp.down_proj': 0.0, 'model.layers.30.mlp.down_proj_mean': array(0.001219, dtype=float16), 'model.layers.30.mlp.down_proj_sigma': array(2.76, dtype=float16)}, {'model.layers.31.self_attn.q_proj': 0.0, 'model.layers.31.self_attn.q_proj_mean': array(-0.01323, dtype=float16), 'model.layers.31.self_attn.q_proj_sigma': array(3.072, dtype=float16), 'model.layers.31.self_attn.k_proj': 0.0, 'model.layers.31.self_attn.k_proj_mean': array(-0.01323, dtype=float16), 'model.layers.31.self_attn.k_proj_sigma': array(3.072, dtype=float16), 'model.layers.31.self_attn.v_proj': 0.0, 'model.layers.31.self_attn.v_proj_mean': array(-0.01323, dtype=float16), 'model.layers.31.self_attn.v_proj_sigma': array(3.072, dtype=float16), 'model.layers.31.self_attn.o_proj': 0.0, 'model.layers.31.self_attn.o_proj_mean': array(-0.004623, dtype=float16), 'model.layers.31.self_attn.o_proj_sigma': array(1.06, dtype=float16), 'model.layers.31.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.31.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.0001046992838382721, 'model.layers.31.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 67.23094525588644, 'model.layers.31.self_attn.s_quantizer': 0.0, 'model.layers.31.self_attn.s_quantizer_mean': -0.06721454858779907, 'model.layers.31.self_attn.s_quantizer_sigma': 38.03945320322047, 'model.layers.31.mlp.gate_proj': 0.0, 'model.layers.31.mlp.gate_proj_mean': array(-0.005913, dtype=float16), 'model.layers.31.mlp.gate_proj_sigma': array(2.693, dtype=float16), 'model.layers.31.mlp.up_proj': 0.0, 'model.layers.31.mlp.up_proj_mean': array(-0.005913, dtype=float16), 'model.layers.31.mlp.up_proj_sigma': array(2.693, dtype=float16), 'model.layers.31.mlp.down_proj': 0.0, 'model.layers.31.mlp.down_proj_mean': array(0.005394, dtype=float16), 'model.layers.31.mlp.down_proj_sigma': array(2.38, dtype=float16)}]
Component-wise statistics across all 32 layers:
Component				Min_K		Max_P
--------------------------------------------------------------------------------
mlp.down_proj                           	0.000000e+00
mlp.down_proj_mean                      	9.933472e-03
mlp.down_proj_sigma                     	2.935547e+00
mlp.gate_proj                           	0.000000e+00
mlp.gate_proj_mean                      	1.094055e-02
mlp.gate_proj_sigma                     	3.076172e+00
mlp.up_proj                             	0.000000e+00
mlp.up_proj_mean                        	1.094055e-02
mlp.up_proj_sigma                       	3.076172e+00
self_attn.apply_rotary_pos_emb_qk_rotation_wrapper	0.000000e+00
self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean	2.375096e-04
self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma	7.696753e+01
self_attn.k_proj                        	0.000000e+00
self_attn.k_proj_mean                   	5.924225e-03
self_attn.k_proj_sigma                  	3.072266e+00
self_attn.o_proj                        	0.000000e+00
self_attn.o_proj_mean                   	1.365662e-02
self_attn.o_proj_sigma                  	2.001953e+00
self_attn.q_proj                        	0.000000e+00
self_attn.q_proj_mean                   	5.924225e-03
self_attn.q_proj_sigma                  	3.072266e+00
self_attn.s_quantizer                   	0.000000e+00
self_attn.s_quantizer_mean              	9.383857e-02
self_attn.s_quantizer_sigma             	8.126500e+01
self_attn.v_proj                        	0.000000e+00
self_attn.v_proj_mean                   	5.924225e-03
self_attn.v_proj_sigma                  	3.072266e+00
Global maximum p across all components: 81.26499861564018
{'global_max_p': 81.26499861564018, 'component_stats': {'mlp.down_proj': {'max_p': 0.0}, 'mlp.down_proj_mean': {'max_p': array(0.00993, dtype=float16)}, 'mlp.down_proj_sigma': {'max_p': array(2.936, dtype=float16)}, 'mlp.gate_proj': {'max_p': 0.0}, 'mlp.gate_proj_mean': {'max_p': array(0.01094, dtype=float16)}, 'mlp.gate_proj_sigma': {'max_p': array(3.076, dtype=float16)}, 'mlp.up_proj': {'max_p': 0.0}, 'mlp.up_proj_mean': {'max_p': array(0.01094, dtype=float16)}, 'mlp.up_proj_sigma': {'max_p': array(3.076, dtype=float16)}, 'self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': {'max_p': 0.0}, 'self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': {'max_p': 0.0002375096082687378}, 'self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': {'max_p': 76.96752561957543}, 'self_attn.k_proj': {'max_p': 0.0}, 'self_attn.k_proj_mean': {'max_p': array(0.005924, dtype=float16)}, 'self_attn.k_proj_sigma': {'max_p': array(3.072, dtype=float16)}, 'self_attn.o_proj': {'max_p': 0.0}, 'self_attn.o_proj_mean': {'max_p': array(0.01366, dtype=float16)}, 'self_attn.o_proj_sigma': {'max_p': array(2.002, dtype=float16)}, 'self_attn.q_proj': {'max_p': 0.0}, 'self_attn.q_proj_mean': {'max_p': array(0.005924, dtype=float16)}, 'self_attn.q_proj_sigma': {'max_p': array(3.072, dtype=float16)}, 'self_attn.s_quantizer': {'max_p': 0.0}, 'self_attn.s_quantizer_mean': {'max_p': 0.09383857250213623}, 'self_attn.s_quantizer_sigma': {'max_p': 81.26499861564018}, 'self_attn.v_proj': {'max_p': 0.0}, 'self_attn.v_proj_mean': {'max_p': array(0.005924, dtype=float16)}, 'self_attn.v_proj_sigma': {'max_p': array(3.072, dtype=float16)}}}
