[2026-01-05 13:29:47 root](check_overflow.py 307): INFO args: Namespace(quant_model_path='./pre_quantized_models/Llama-3-8B-w4a4q4s8kv4', output_dir='./log/test', real_quant=False, ppl_seqlen=2048, seed=2, eval_ppl=True, eval_tasks='', eval_batch_size=16, max_memory='70GiB')
[2026-01-05 13:30:07 root](check_overflow.py 342): INFO init weight quantizer
[2026-01-05 13:30:07 root](check_overflow.py 347): INFO init input quantizer
[2026-01-05 13:30:07 root](check_overflow.py 352): INFO init v quantizer
[2026-01-05 13:30:07 root](check_overflow.py 358): INFO init k quantizer
q and k-cache quantization: set model.layers.0.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.1.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.2.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.3.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.4.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.5.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.6.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.7.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.8.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.9.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.10.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.11.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.12.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.13.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.14.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.15.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.16.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.17.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.18.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.19.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.20.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.21.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.22.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.23.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.24.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.25.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.26.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.27.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.28.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.29.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.30.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
q and k-cache quantization: set model.layers.31.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper as 4-bit 128 groupsize static symmetric quantization
[2026-01-05 13:30:07 root](check_overflow.py 362): INFO init s quantizer
Loading pre-computed quantized weights...
[2026-01-05 13:31:05 root](check_overflow.py 370): INFO LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): QuantLinear(
            (weight_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=torch.Size([4096, 4096])
              group_size=4096
              quant_type='weight'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(4096, 1)
              zero_point=None
            )
          )
          (k_proj): QuantLinear(
            (weight_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=torch.Size([1024, 4096])
              group_size=4096
              quant_type='weight'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(1024, 1)
              zero_point=None
            )
          )
          (v_proj): QuantLinear(
            (weight_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=torch.Size([1024, 4096])
              group_size=4096
              quant_type='weight'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(1024, 1)
              zero_point=None
            )
            (output_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=(1, 1024)
              group_size=128
              quant_type='activation'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(8, 1)
              zero_point=None
            )
          )
          (o_proj): QuantLinear(
            (weight_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=torch.Size([4096, 4096])
              group_size=4096
              quant_type='weight'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(4096, 1)
              zero_point=None
            )
            (input_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=(1, 4096)
              group_size=4096
              quant_type='activation'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(1, 1)
              zero_point=None
            )
          )
          (rotary_emb): LlamaRotaryEmbedding()
          (apply_rotary_pos_emb_qk_rotation_wrapper): QKRotationWrapper(
            (k_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=(1, 1024)
              group_size=128
              quant_type='activation'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(8, 1)
              zero_point=None
            )
            (q_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=(1, 4096)
              group_size=128
              quant_type='activation'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(32, 1)
              zero_point=None
            )
          )
          (s_quantizer): UniformAffineQuantizer(
            n_bits=8
            quantized_shape=(1, 1)
            group_size=1
            quant_type='activation'
            mode='static'
            asym=False
            disable_zero_point_in_sym=True
            activation_clipping=False
            enable=True
            is_s=True
            qmin=-128, qmax=127
            scale_shape=(32, 1, 1)
            zero_point=None
          )
        )
        (mlp): LlamaMLP(
          (gate_proj): QuantLinear(
            (weight_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=torch.Size([14336, 4096])
              group_size=4096
              quant_type='weight'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(14336, 1)
              zero_point=None
            )
          )
          (up_proj): QuantLinear(
            (weight_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=torch.Size([14336, 4096])
              group_size=4096
              quant_type='weight'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(14336, 1)
              zero_point=None
            )
          )
          (down_proj): QuantLinear(
            (weight_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=torch.Size([4096, 14336])
              group_size=14336
              quant_type='weight'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(4096, 1)
              zero_point=None
            )
            (input_quantizer): UniformAffineQuantizer(
              n_bits=4
              quantized_shape=(1, 14336)
              group_size=14336
              quant_type='activation'
              mode='static'
              asym=False
              disable_zero_point_in_sym=True
              activation_clipping=False
              enable=True
              is_s=False
              qmin=-8, qmax=7
              scale_shape=(1, 1)
              zero_point=None
            )
          )
          (act_fn): SiLU()
        )
        (input_layernorm): QuantRMSNorm(
          (output_quantizer): UniformAffineQuantizer(
            n_bits=4
            quantized_shape=(1, 4096)
            group_size=4096
            quant_type='activation'
            mode='static'
            asym=False
            disable_zero_point_in_sym=True
            activation_clipping=False
            enable=True
            is_s=False
            qmin=-8, qmax=7
            scale_shape=(1, 1)
            zero_point=None
          )
        )
        (post_attention_layernorm): QuantRMSNorm(
          (output_quantizer): UniformAffineQuantizer(
            n_bits=4
            quantized_shape=(1, 4096)
            group_size=4096
            quant_type='activation'
            mode='static'
            asym=False
            disable_zero_point_in_sym=True
            activation_clipping=False
            enable=True
            is_s=False
            qmin=-8, qmax=7
            scale_shape=(1, 1)
            zero_point=None
          )
        )
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
name: model.layers.0.self_attn.v_proj.output_quantizer
name: model.layers.0.self_attn.o_proj.input_quantizer
name: model.layers.0.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.0.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.0.self_attn.s_quantizer
name: model.layers.0.mlp.down_proj.input_quantizer
name: model.layers.0.input_layernorm.output_quantizer
name: model.layers.0.post_attention_layernorm.output_quantizer
name: model.layers.1.self_attn.v_proj.output_quantizer
name: model.layers.1.self_attn.o_proj.input_quantizer
name: model.layers.1.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.1.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.1.self_attn.s_quantizer
name: model.layers.1.mlp.down_proj.input_quantizer
name: model.layers.1.input_layernorm.output_quantizer
name: model.layers.1.post_attention_layernorm.output_quantizer
name: model.layers.2.self_attn.v_proj.output_quantizer
name: model.layers.2.self_attn.o_proj.input_quantizer
name: model.layers.2.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.2.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.2.self_attn.s_quantizer
name: model.layers.2.mlp.down_proj.input_quantizer
name: model.layers.2.input_layernorm.output_quantizer
name: model.layers.2.post_attention_layernorm.output_quantizer
name: model.layers.3.self_attn.v_proj.output_quantizer
name: model.layers.3.self_attn.o_proj.input_quantizer
name: model.layers.3.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.3.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.3.self_attn.s_quantizer
name: model.layers.3.mlp.down_proj.input_quantizer
name: model.layers.3.input_layernorm.output_quantizer
name: model.layers.3.post_attention_layernorm.output_quantizer
name: model.layers.4.self_attn.v_proj.output_quantizer
name: model.layers.4.self_attn.o_proj.input_quantizer
name: model.layers.4.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.4.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.4.self_attn.s_quantizer
name: model.layers.4.mlp.down_proj.input_quantizer
name: model.layers.4.input_layernorm.output_quantizer
name: model.layers.4.post_attention_layernorm.output_quantizer
name: model.layers.5.self_attn.v_proj.output_quantizer
name: model.layers.5.self_attn.o_proj.input_quantizer
name: model.layers.5.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.5.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.5.self_attn.s_quantizer
name: model.layers.5.mlp.down_proj.input_quantizer
name: model.layers.5.input_layernorm.output_quantizer
name: model.layers.5.post_attention_layernorm.output_quantizer
name: model.layers.6.self_attn.v_proj.output_quantizer
name: model.layers.6.self_attn.o_proj.input_quantizer
name: model.layers.6.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.6.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.6.self_attn.s_quantizer
name: model.layers.6.mlp.down_proj.input_quantizer
name: model.layers.6.input_layernorm.output_quantizer
name: model.layers.6.post_attention_layernorm.output_quantizer
name: model.layers.7.self_attn.v_proj.output_quantizer
name: model.layers.7.self_attn.o_proj.input_quantizer
name: model.layers.7.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.7.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.7.self_attn.s_quantizer
name: model.layers.7.mlp.down_proj.input_quantizer
name: model.layers.7.input_layernorm.output_quantizer
name: model.layers.7.post_attention_layernorm.output_quantizer
name: model.layers.8.self_attn.v_proj.output_quantizer
name: model.layers.8.self_attn.o_proj.input_quantizer
name: model.layers.8.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.8.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.8.self_attn.s_quantizer
name: model.layers.8.mlp.down_proj.input_quantizer
name: model.layers.8.input_layernorm.output_quantizer
name: model.layers.8.post_attention_layernorm.output_quantizer
name: model.layers.9.self_attn.v_proj.output_quantizer
name: model.layers.9.self_attn.o_proj.input_quantizer
name: model.layers.9.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.9.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.9.self_attn.s_quantizer
name: model.layers.9.mlp.down_proj.input_quantizer
name: model.layers.9.input_layernorm.output_quantizer
name: model.layers.9.post_attention_layernorm.output_quantizer
name: model.layers.10.self_attn.v_proj.output_quantizer
name: model.layers.10.self_attn.o_proj.input_quantizer
name: model.layers.10.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.10.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.10.self_attn.s_quantizer
name: model.layers.10.mlp.down_proj.input_quantizer
name: model.layers.10.input_layernorm.output_quantizer
name: model.layers.10.post_attention_layernorm.output_quantizer
name: model.layers.11.self_attn.v_proj.output_quantizer
name: model.layers.11.self_attn.o_proj.input_quantizer
name: model.layers.11.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.11.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.11.self_attn.s_quantizer
name: model.layers.11.mlp.down_proj.input_quantizer
name: model.layers.11.input_layernorm.output_quantizer
name: model.layers.11.post_attention_layernorm.output_quantizer
name: model.layers.12.self_attn.v_proj.output_quantizer
name: model.layers.12.self_attn.o_proj.input_quantizer
name: model.layers.12.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.12.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.12.self_attn.s_quantizer
name: model.layers.12.mlp.down_proj.input_quantizer
name: model.layers.12.input_layernorm.output_quantizer
name: model.layers.12.post_attention_layernorm.output_quantizer
name: model.layers.13.self_attn.v_proj.output_quantizer
name: model.layers.13.self_attn.o_proj.input_quantizer
name: model.layers.13.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.13.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.13.self_attn.s_quantizer
name: model.layers.13.mlp.down_proj.input_quantizer
name: model.layers.13.input_layernorm.output_quantizer
name: model.layers.13.post_attention_layernorm.output_quantizer
name: model.layers.14.self_attn.v_proj.output_quantizer
name: model.layers.14.self_attn.o_proj.input_quantizer
name: model.layers.14.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.14.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.14.self_attn.s_quantizer
name: model.layers.14.mlp.down_proj.input_quantizer
name: model.layers.14.input_layernorm.output_quantizer
name: model.layers.14.post_attention_layernorm.output_quantizer
name: model.layers.15.self_attn.v_proj.output_quantizer
name: model.layers.15.self_attn.o_proj.input_quantizer
name: model.layers.15.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.15.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.15.self_attn.s_quantizer
name: model.layers.15.mlp.down_proj.input_quantizer
name: model.layers.15.input_layernorm.output_quantizer
name: model.layers.15.post_attention_layernorm.output_quantizer
name: model.layers.16.self_attn.v_proj.output_quantizer
name: model.layers.16.self_attn.o_proj.input_quantizer
name: model.layers.16.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.16.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.16.self_attn.s_quantizer
name: model.layers.16.mlp.down_proj.input_quantizer
name: model.layers.16.input_layernorm.output_quantizer
name: model.layers.16.post_attention_layernorm.output_quantizer
name: model.layers.17.self_attn.v_proj.output_quantizer
name: model.layers.17.self_attn.o_proj.input_quantizer
name: model.layers.17.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.17.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.17.self_attn.s_quantizer
name: model.layers.17.mlp.down_proj.input_quantizer
name: model.layers.17.input_layernorm.output_quantizer
name: model.layers.17.post_attention_layernorm.output_quantizer
name: model.layers.18.self_attn.v_proj.output_quantizer
name: model.layers.18.self_attn.o_proj.input_quantizer
name: model.layers.18.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.18.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.18.self_attn.s_quantizer
name: model.layers.18.mlp.down_proj.input_quantizer
name: model.layers.18.input_layernorm.output_quantizer
name: model.layers.18.post_attention_layernorm.output_quantizer
name: model.layers.19.self_attn.v_proj.output_quantizer
name: model.layers.19.self_attn.o_proj.input_quantizer
name: model.layers.19.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.19.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.19.self_attn.s_quantizer
name: model.layers.19.mlp.down_proj.input_quantizer
name: model.layers.19.input_layernorm.output_quantizer
name: model.layers.19.post_attention_layernorm.output_quantizer
name: model.layers.20.self_attn.v_proj.output_quantizer
name: model.layers.20.self_attn.o_proj.input_quantizer
name: model.layers.20.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.20.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.20.self_attn.s_quantizer
name: model.layers.20.mlp.down_proj.input_quantizer
name: model.layers.20.input_layernorm.output_quantizer
name: model.layers.20.post_attention_layernorm.output_quantizer
name: model.layers.21.self_attn.v_proj.output_quantizer
name: model.layers.21.self_attn.o_proj.input_quantizer
name: model.layers.21.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.21.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.21.self_attn.s_quantizer
name: model.layers.21.mlp.down_proj.input_quantizer
name: model.layers.21.input_layernorm.output_quantizer
name: model.layers.21.post_attention_layernorm.output_quantizer
name: model.layers.22.self_attn.v_proj.output_quantizer
name: model.layers.22.self_attn.o_proj.input_quantizer
name: model.layers.22.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.22.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.22.self_attn.s_quantizer
name: model.layers.22.mlp.down_proj.input_quantizer
name: model.layers.22.input_layernorm.output_quantizer
name: model.layers.22.post_attention_layernorm.output_quantizer
name: model.layers.23.self_attn.v_proj.output_quantizer
name: model.layers.23.self_attn.o_proj.input_quantizer
name: model.layers.23.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.23.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.23.self_attn.s_quantizer
name: model.layers.23.mlp.down_proj.input_quantizer
name: model.layers.23.input_layernorm.output_quantizer
name: model.layers.23.post_attention_layernorm.output_quantizer
name: model.layers.24.self_attn.v_proj.output_quantizer
name: model.layers.24.self_attn.o_proj.input_quantizer
name: model.layers.24.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.24.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.24.self_attn.s_quantizer
name: model.layers.24.mlp.down_proj.input_quantizer
name: model.layers.24.input_layernorm.output_quantizer
name: model.layers.24.post_attention_layernorm.output_quantizer
name: model.layers.25.self_attn.v_proj.output_quantizer
name: model.layers.25.self_attn.o_proj.input_quantizer
name: model.layers.25.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.25.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.25.self_attn.s_quantizer
name: model.layers.25.mlp.down_proj.input_quantizer
name: model.layers.25.input_layernorm.output_quantizer
name: model.layers.25.post_attention_layernorm.output_quantizer
name: model.layers.26.self_attn.v_proj.output_quantizer
name: model.layers.26.self_attn.o_proj.input_quantizer
name: model.layers.26.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.26.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.26.self_attn.s_quantizer
name: model.layers.26.mlp.down_proj.input_quantizer
name: model.layers.26.input_layernorm.output_quantizer
name: model.layers.26.post_attention_layernorm.output_quantizer
name: model.layers.27.self_attn.v_proj.output_quantizer
name: model.layers.27.self_attn.o_proj.input_quantizer
name: model.layers.27.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.27.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.27.self_attn.s_quantizer
name: model.layers.27.mlp.down_proj.input_quantizer
name: model.layers.27.input_layernorm.output_quantizer
name: model.layers.27.post_attention_layernorm.output_quantizer
name: model.layers.28.self_attn.v_proj.output_quantizer
name: model.layers.28.self_attn.o_proj.input_quantizer
name: model.layers.28.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.28.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.28.self_attn.s_quantizer
name: model.layers.28.mlp.down_proj.input_quantizer
name: model.layers.28.input_layernorm.output_quantizer
name: model.layers.28.post_attention_layernorm.output_quantizer
name: model.layers.29.self_attn.v_proj.output_quantizer
name: model.layers.29.self_attn.o_proj.input_quantizer
name: model.layers.29.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.29.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.29.self_attn.s_quantizer
name: model.layers.29.mlp.down_proj.input_quantizer
name: model.layers.29.input_layernorm.output_quantizer
name: model.layers.29.post_attention_layernorm.output_quantizer
name: model.layers.30.self_attn.v_proj.output_quantizer
name: model.layers.30.self_attn.o_proj.input_quantizer
name: model.layers.30.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.30.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.30.self_attn.s_quantizer
name: model.layers.30.mlp.down_proj.input_quantizer
name: model.layers.30.input_layernorm.output_quantizer
name: model.layers.30.post_attention_layernorm.output_quantizer
name: model.layers.31.self_attn.v_proj.output_quantizer
name: model.layers.31.self_attn.o_proj.input_quantizer
name: model.layers.31.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.k_quantizer
name: model.layers.31.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper.q_quantizer
name: model.layers.31.self_attn.s_quantizer
name: model.layers.31.mlp.down_proj.input_quantizer
name: model.layers.31.input_layernorm.output_quantizer
name: model.layers.31.post_attention_layernorm.output_quantizer
get_wikitext2
wikitext2:8.719918251037598
[2026-01-05 13:33:12 root](main.py 33): INFO wikitext2 perplexity: 8.72
[{'qkv_var': tensor(5.7891, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0087, device='cuda:0', dtype=torch.float16), 'v_var': tensor(5.6680, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0097, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.5000, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0088, device='cuda:0', dtype=torch.float16), 'q_var': tensor(3.1191, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(0.0001, device='cuda:0', dtype=torch.float16), 's_var': tensor(3.5430, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.2258, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.5156, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0047, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(8.9844, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0082, device='cuda:0', dtype=torch.float16), 'down_var': tensor(5.6641, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0011, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(6.3242, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0034, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.8906, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0191, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.7734, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0095, device='cuda:0', dtype=torch.float16), 'q_var': tensor(3.8496, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(5.0664e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(3.9199, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1115, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.7324, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0115, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.7109, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0016, device='cuda:0', dtype=torch.float16), 'down_var': tensor(5.2812, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0016, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(6.6562, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0033, device='cuda:0', dtype=torch.float16), 'v_var': tensor(8.0312, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(0.0010, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.9453, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0103, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1055, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(6.3717e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(4.0625, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1086, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.1836, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0198, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(9.2969, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0040, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.7852, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(0.0016, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(6.8438, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0002, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.2305, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(0.0062, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.5625, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0103, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-9.1493e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(3.0840, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1055, device='cuda:0', dtype=torch.float16), 'o_var': tensor(3.4648, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(0.0023, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(8.5391, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0020, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.0508, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-5.7340e-05, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(6.8438, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0017, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.6133, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0169, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.1719, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0103, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1055, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-1.5616e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(3.3730, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1127, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.7285, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0273, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.3594, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0004, device='cuda:0', dtype=torch.float16), 'down_var': tensor(7.2266, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0026, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(5.9258, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(9.8944e-06, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.2305, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0020, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.8750, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0104, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.2227, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-4.4346e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(3.1016, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1045, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.5430, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0153, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.5508, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0053, device='cuda:0', dtype=torch.float16), 'down_var': tensor(7.4180, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(0.0017, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.0781, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0088, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.3789, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0076, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.0156, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0090, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1055, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(4.7624e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.7285, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1114, device='cuda:0', dtype=torch.float16), 'o_var': tensor(3.4785, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0319, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.3711, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0014, device='cuda:0', dtype=torch.float16), 'down_var': tensor(7.2266, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0043, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.6172, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0042, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.7266, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0047, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.6094, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0120, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.3281, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-0.0001, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.6328, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1279, device='cuda:0', dtype=torch.float16), 'o_var': tensor(4.4883, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0047, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.7148, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0056, device='cuda:0', dtype=torch.float16), 'down_var': tensor(7.1992, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(0.0012, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.7656, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0051, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.6133, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(0.0049, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.5469, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0135, device='cuda:0', dtype=torch.float16), 'q_var': tensor(3.9551, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-1.9431e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.9805, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1472, device='cuda:0', dtype=torch.float16), 'o_var': tensor(4.3398, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0148, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.8594, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0008, device='cuda:0', dtype=torch.float16), 'down_var': tensor(8.2109, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0023, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(6.6992, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0002, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.2305, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0131, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.5469, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0106, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.2227, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-5.0306e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(3.0664, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1766, device='cuda:0', dtype=torch.float16), 'o_var': tensor(4.8867, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0154, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.7695, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0041, device='cuda:0', dtype=torch.float16), 'down_var': tensor(8.2188, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0075, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(5.8711, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0017, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.6172, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(0.0164, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.5469, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0114, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(7.7486e-06, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.8242, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1334, device='cuda:0', dtype=torch.float16), 'o_var': tensor(4.9922, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(0.0079, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.3359, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0012, device='cuda:0', dtype=torch.float16), 'down_var': tensor(8.5078, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0065, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.3164, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0003, device='cuda:0', dtype=torch.float16), 'v_var': tensor(8.0156, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0215, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.7812, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0103, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.3867, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-0.0001, device='cuda:0', dtype=torch.float16), 's_var': tensor(3.2070, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1759, device='cuda:0', dtype=torch.float16), 'o_var': tensor(4.4766, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0425, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.3789, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-4.9710e-05, device='cuda:0', dtype=torch.float16), 'down_var': tensor(8.0156, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0054, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.4609, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0050, device='cuda:0', dtype=torch.float16), 'v_var': tensor(8., device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0100, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.5469, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0103, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.4922, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-9.1434e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(3.9980, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1566, device='cuda:0', dtype=torch.float16), 'o_var': tensor(4.4883, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0158, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.5195, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0070, device='cuda:0', dtype=torch.float16), 'down_var': tensor(8.2109, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0035, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.7227, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0166, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.6172, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0199, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.0859, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0103, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.4883, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(3.0398e-06, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.9688, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1490, device='cuda:0', dtype=torch.float16), 'o_var': tensor(5.2695, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0241, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(8.0469, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0092, device='cuda:0', dtype=torch.float16), 'down_var': tensor(7.2305, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0010, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.0625, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0182, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.6133, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0075, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.5469, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0118, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.4219, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(1.9789e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.6973, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1201, device='cuda:0', dtype=torch.float16), 'o_var': tensor(3.5410, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0128, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.7461, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0138, device='cuda:0', dtype=torch.float16), 'down_var': tensor(8.2109, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0034, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.2266, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0075, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.2305, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0220, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.7812, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0088, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.3555, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(4.0412e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.5059, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1222, device='cuda:0', dtype=torch.float16), 'o_var': tensor(5.0430, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0406, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.0781, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0125, device='cuda:0', dtype=torch.float16), 'down_var': tensor(8.3125, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0046, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.5469, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0132, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.6133, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0133, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.9453, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0118, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.2734, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-8.1062e-06, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.3691, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0973, device='cuda:0', dtype=torch.float16), 'o_var': tensor(4.4688, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0034, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.1172, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0029, device='cuda:0', dtype=torch.float16), 'down_var': tensor(8.0625, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0038, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(6.9688, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0007, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.6641, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(0.0145, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.6172, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0110, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.6211, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-5.4300e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.5840, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1282, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.4492, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(0.0190, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.4102, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0022, device='cuda:0', dtype=torch.float16), 'down_var': tensor(8.0156, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(0.0052, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(5.9023, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0007, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.8359, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(0.0083, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.5703, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0111, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.3438, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-2.7120e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.1133, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0939, device='cuda:0', dtype=torch.float16), 'o_var': tensor(3.4199, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0039, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.4961, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0026, device='cuda:0', dtype=torch.float16), 'down_var': tensor(7.6328, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0028, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(6.7148, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0016, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.5078, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(0.0195, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.0156, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0100, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.3203, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-3.7611e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.3711, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.1052, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.2207, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(0.0058, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(9.0156, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0050, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.1953, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(0.0006, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(6.3047, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0019, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.1484, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(0.0043, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.8438, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0093, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.0078, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-9.5367e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.7627, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0833, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.1641, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0059, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(8.3125, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0028, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.1719, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0062, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(6.5664, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(6.9141e-06, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.2305, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0283, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.4922, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0109, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.4883, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-3.2365e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.2285, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0764, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.1777, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0134, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(9.1562, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0036, device='cuda:0', dtype=torch.float16), 'down_var': tensor(5.7578, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(0.0002, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(6.9414, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0009, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.3672, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0019, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.8672, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0096, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.4883, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-1.1504e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.6748, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0648, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.0645, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0115, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(9.1797, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0052, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.2383, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0028, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(6.7656, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0027, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.1094, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0074, device='cuda:0', dtype=torch.float16), 'k_var': tensor(11.0859, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0137, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.3867, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(0.0001, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.7773, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0650, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.2314, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(0.0009, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(8.9688, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0102, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.0195, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0015, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(6.6328, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0054, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.4883, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0202, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.5469, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0117, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.5352, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-0.0002, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.4355, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0518, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.5010, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(0.0067, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.0430, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0071, device='cuda:0', dtype=torch.float16), 'down_var': tensor(8.0547, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0107, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.2969, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0046, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.8438, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0132, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.5469, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0096, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.5781, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-4.4823e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.9746, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0563, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.8877, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0281, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.4805, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0042, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.1328, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0026, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.0742, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0026, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.5938, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0257, device='cuda:0', dtype=torch.float16), 'k_var': tensor(10.5469, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0114, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.4922, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-7.6115e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.9434, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0679, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.0449, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0206, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(9.5078, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0065, device='cuda:0', dtype=torch.float16), 'down_var': tensor(5.7812, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0009, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.3398, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0004, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.6016, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(0.0069, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.1641, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0079, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.8867, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-9.7096e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.7490, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0682, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.9336, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(0.0005, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(9.7422, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0093, device='cuda:0', dtype=torch.float16), 'down_var': tensor(8.8203, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0128, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.7734, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0007, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.0039, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(0.0112, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.7891, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0089, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.0078, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(6.7115e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.4932, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0576, device='cuda:0', dtype=torch.float16), 'o_var': tensor(2.2793, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0084, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.7773, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0019, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.0859, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0051, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.6016, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0093, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.7500, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0168, device='cuda:0', dtype=torch.float16), 'k_var': tensor(11.3125, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0105, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1133, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-6.9976e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.6553, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0742, device='cuda:0', dtype=torch.float16), 'o_var': tensor(3.9023, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0285, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.9961, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(0.0039, device='cuda:0', dtype=torch.float16), 'down_var': tensor(6.0352, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0002, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(7.6172, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(-0.0002, device='cuda:0', dtype=torch.float16), 'v_var': tensor(6.7773, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(0.0576, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.8359, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0103, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.2734, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(4.6790e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(1.8066, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0995, device='cuda:0', dtype=torch.float16), 'o_var': tensor(3.2383, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(0.0441, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(8.3125, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0088, device='cuda:0', dtype=torch.float16), 'down_var': tensor(8.5234, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0059, device='cuda:0', dtype=torch.float16)}, {'qkv_var': tensor(5.7305, device='cuda:0', dtype=torch.float16), 'qkv_mean': tensor(0.0057, device='cuda:0', dtype=torch.float16), 'v_var': tensor(7.2305, device='cuda:0', dtype=torch.float16), 'v_mean': tensor(-0.0099, device='cuda:0', dtype=torch.float16), 'k_var': tensor(9.9453, device='cuda:0', dtype=torch.float16), 'k_mean': tensor(-0.0078, device='cuda:0', dtype=torch.float16), 'q_var': tensor(4.1016, device='cuda:0', dtype=torch.float16), 'q_mean': tensor(-9.0063e-05, device='cuda:0', dtype=torch.float16), 's_var': tensor(2.5488, device='cuda:0', dtype=torch.float16), 's_mean': tensor(0.0822, device='cuda:0', dtype=torch.float16), 'o_var': tensor(1.1250, device='cuda:0', dtype=torch.float16), 'o_mean': tensor(-0.0085, device='cuda:0', dtype=torch.float16), 'up_gate_var': tensor(7.1562, device='cuda:0', dtype=torch.float16), 'up_gate_mean': tensor(-0.0051, device='cuda:0', dtype=torch.float16), 'down_var': tensor(4.4883, device='cuda:0', dtype=torch.float16), 'down_mean': tensor(-0.0038, device='cuda:0', dtype=torch.float16)}]
[{'model.layers.0.self_attn.q_proj': 0.0, 'model.layers.0.self_attn.q_proj_mean': array(0.008736, dtype=float16), 'model.layers.0.self_attn.q_proj_sigma': array(2.406, dtype=float16), 'model.layers.0.self_attn.k_proj': 0.0, 'model.layers.0.self_attn.k_proj_mean': array(0.008736, dtype=float16), 'model.layers.0.self_attn.k_proj_sigma': array(2.406, dtype=float16), 'model.layers.0.self_attn.v_proj': 0.0, 'model.layers.0.self_attn.v_proj_mean': array(0.008736, dtype=float16), 'model.layers.0.self_attn.v_proj_sigma': array(2.406, dtype=float16), 'model.layers.0.self_attn.o_proj': 0.0, 'model.layers.0.self_attn.o_proj_mean': array(-0.00466, dtype=float16), 'model.layers.0.self_attn.o_proj_sigma': array(1.231, dtype=float16), 'model.layers.0.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.0.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -0.00011707597877830267, 'model.layers.0.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 61.56297588648554, 'model.layers.0.self_attn.s_quantizer': 0.0, 'model.layers.0.self_attn.s_quantizer_mean': -0.28096437454223633, 'model.layers.0.self_attn.s_quantizer_sigma': 51.068581339214816, 'model.layers.0.mlp.gate_proj': 0.0, 'model.layers.0.mlp.gate_proj_mean': array(-0.00819, dtype=float16), 'model.layers.0.mlp.gate_proj_sigma': array(2.998, dtype=float16), 'model.layers.0.mlp.up_proj': 0.0, 'model.layers.0.mlp.up_proj_mean': array(-0.00819, dtype=float16), 'model.layers.0.mlp.up_proj_sigma': array(2.998, dtype=float16), 'model.layers.0.mlp.down_proj': 0.0, 'model.layers.0.mlp.down_proj_mean': array(-0.001148, dtype=float16), 'model.layers.0.mlp.down_proj_sigma': array(2.38, dtype=float16)}, {'model.layers.1.self_attn.q_proj': 0.0, 'model.layers.1.self_attn.q_proj_mean': array(0.00336, dtype=float16), 'model.layers.1.self_attn.q_proj_sigma': array(2.516, dtype=float16), 'model.layers.1.self_attn.k_proj': 0.0, 'model.layers.1.self_attn.k_proj_mean': array(0.00336, dtype=float16), 'model.layers.1.self_attn.k_proj_sigma': array(2.516, dtype=float16), 'model.layers.1.self_attn.v_proj': 0.0, 'model.layers.1.self_attn.v_proj_mean': array(0.00336, dtype=float16), 'model.layers.1.self_attn.v_proj_sigma': array(2.516, dtype=float16), 'model.layers.1.self_attn.o_proj': 0.0, 'model.layers.1.self_attn.o_proj_mean': array(-0.011505, dtype=float16), 'model.layers.1.self_attn.o_proj_sigma': array(1.316, dtype=float16), 'model.layers.1.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.1.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -6.13013980910182e-05, 'model.layers.1.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 69.42621983083913, 'model.layers.1.self_attn.s_quantizer': 0.0, 'model.layers.1.self_attn.s_quantizer_mean': -0.2718775272369385, 'model.layers.1.self_attn.s_quantizer_sigma': 58.872744118140105, 'model.layers.1.mlp.gate_proj': 0.0, 'model.layers.1.mlp.gate_proj_mean': array(0.00163, dtype=float16), 'model.layers.1.mlp.gate_proj_sigma': array(2.777, dtype=float16), 'model.layers.1.mlp.up_proj': 0.0, 'model.layers.1.mlp.up_proj_mean': array(0.00163, dtype=float16), 'model.layers.1.mlp.up_proj_sigma': array(2.777, dtype=float16), 'model.layers.1.mlp.down_proj': 0.0, 'model.layers.1.mlp.down_proj_mean': array(-0.001613, dtype=float16), 'model.layers.1.mlp.down_proj_sigma': array(2.299, dtype=float16)}, {'model.layers.2.self_attn.q_proj': 0.0, 'model.layers.2.self_attn.q_proj_mean': array(0.003342, dtype=float16), 'model.layers.2.self_attn.q_proj_sigma': array(2.58, dtype=float16), 'model.layers.2.self_attn.k_proj': 0.0, 'model.layers.2.self_attn.k_proj_mean': array(0.003342, dtype=float16), 'model.layers.2.self_attn.k_proj_sigma': array(2.58, dtype=float16), 'model.layers.2.self_attn.v_proj': 0.0, 'model.layers.2.self_attn.v_proj_mean': array(0.003342, dtype=float16), 'model.layers.2.self_attn.v_proj_sigma': array(2.58, dtype=float16), 'model.layers.2.self_attn.o_proj': 0.0, 'model.layers.2.self_attn.o_proj_mean': array(-0.0198, dtype=float16), 'model.layers.2.self_attn.o_proj_sigma': array(1.478, dtype=float16), 'model.layers.2.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.2.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -8.3691265899688e-05, 'model.layers.2.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 72.27724399837061, 'model.layers.2.self_attn.s_quantizer': 0.0, 'model.layers.2.self_attn.s_quantizer_mean': 0.014129385352134705, 'model.layers.2.self_attn.s_quantizer_sigma': 64.71475874945375, 'model.layers.2.mlp.gate_proj': 0.0, 'model.layers.2.mlp.gate_proj_mean': array(-0.00398, dtype=float16), 'model.layers.2.mlp.gate_proj_sigma': array(3.049, dtype=float16), 'model.layers.2.mlp.up_proj': 0.0, 'model.layers.2.mlp.up_proj_mean': array(-0.00398, dtype=float16), 'model.layers.2.mlp.up_proj_sigma': array(3.049, dtype=float16), 'model.layers.2.mlp.down_proj': 0.0, 'model.layers.2.mlp.down_proj_mean': array(0.001584, dtype=float16), 'model.layers.2.mlp.down_proj_sigma': array(2.605, dtype=float16)}, {'model.layers.3.self_attn.q_proj': 0.0, 'model.layers.3.self_attn.q_proj_mean': array(-0.0001563, dtype=float16), 'model.layers.3.self_attn.q_proj_sigma': array(2.615, dtype=float16), 'model.layers.3.self_attn.k_proj': 0.0, 'model.layers.3.self_attn.k_proj_mean': array(-0.0001563, dtype=float16), 'model.layers.3.self_attn.k_proj_sigma': array(2.615, dtype=float16), 'model.layers.3.self_attn.v_proj': 0.0, 'model.layers.3.self_attn.v_proj_mean': array(-0.0001563, dtype=float16), 'model.layers.3.self_attn.v_proj_sigma': array(2.615, dtype=float16), 'model.layers.3.self_attn.o_proj': 0.0, 'model.layers.3.self_attn.o_proj_mean': array(0.002277, dtype=float16), 'model.layers.3.self_attn.o_proj_sigma': array(1.861, dtype=float16), 'model.layers.3.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.3.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.00012071017408743501, 'model.layers.3.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 74.45804187594514, 'model.layers.3.self_attn.s_quantizer': 0.0, 'model.layers.3.self_attn.s_quantizer_mean': 0.08414840698242188, 'model.layers.3.self_attn.s_quantizer_sigma': 53.53503525729669, 'model.layers.3.mlp.gate_proj': 0.0, 'model.layers.3.mlp.gate_proj_mean': array(-0.002005, dtype=float16), 'model.layers.3.mlp.gate_proj_sigma': array(2.922, dtype=float16), 'model.layers.3.mlp.up_proj': 0.0, 'model.layers.3.mlp.up_proj_mean': array(-0.002005, dtype=float16), 'model.layers.3.mlp.up_proj_sigma': array(2.922, dtype=float16), 'model.layers.3.mlp.down_proj': 0.0, 'model.layers.3.mlp.down_proj_mean': array(-5.734e-05, dtype=float16), 'model.layers.3.mlp.down_proj_sigma': array(2.459, dtype=float16)}, {'model.layers.4.self_attn.q_proj': 0.0, 'model.layers.4.self_attn.q_proj_mean': array(0.0017185, dtype=float16), 'model.layers.4.self_attn.q_proj_sigma': array(2.615, dtype=float16), 'model.layers.4.self_attn.k_proj': 0.0, 'model.layers.4.self_attn.k_proj_mean': array(0.0017185, dtype=float16), 'model.layers.4.self_attn.k_proj_sigma': array(2.615, dtype=float16), 'model.layers.4.self_attn.v_proj': 0.0, 'model.layers.4.self_attn.v_proj_mean': array(0.0017185, dtype=float16), 'model.layers.4.self_attn.v_proj_sigma': array(2.615, dtype=float16), 'model.layers.4.self_attn.o_proj': 0.0, 'model.layers.4.self_attn.o_proj_mean': array(-0.02728, dtype=float16), 'model.layers.4.self_attn.o_proj_sigma': array(1.651, dtype=float16), 'model.layers.4.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.4.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 2.0633800886571407e-05, 'model.layers.4.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 73.07530362578044, 'model.layers.4.self_attn.s_quantizer': 0.0, 'model.layers.4.self_attn.s_quantizer_mean': -0.24351859092712402, 'model.layers.4.self_attn.s_quantizer_sigma': 57.463031594234565, 'model.layers.4.mlp.gate_proj': 0.0, 'model.layers.4.mlp.gate_proj_mean': array(0.000433, dtype=float16), 'model.layers.4.mlp.gate_proj_sigma': array(2.713, dtype=float16), 'model.layers.4.mlp.up_proj': 0.0, 'model.layers.4.mlp.up_proj_mean': array(0.000433, dtype=float16), 'model.layers.4.mlp.up_proj_sigma': array(2.713, dtype=float16), 'model.layers.4.mlp.down_proj': 0.0, 'model.layers.4.mlp.down_proj_mean': array(-0.002625, dtype=float16), 'model.layers.4.mlp.down_proj_sigma': array(2.688, dtype=float16)}, {'model.layers.5.self_attn.q_proj': 0.0, 'model.layers.5.self_attn.q_proj_mean': array(9.9e-06, dtype=float16), 'model.layers.5.self_attn.q_proj_sigma': array(2.434, dtype=float16), 'model.layers.5.self_attn.k_proj': 0.0, 'model.layers.5.self_attn.k_proj_mean': array(9.9e-06, dtype=float16), 'model.layers.5.self_attn.k_proj_sigma': array(2.434, dtype=float16), 'model.layers.5.self_attn.v_proj': 0.0, 'model.layers.5.self_attn.v_proj_mean': array(9.9e-06, dtype=float16), 'model.layers.5.self_attn.v_proj_sigma': array(2.434, dtype=float16), 'model.layers.5.self_attn.o_proj': 0.0, 'model.layers.5.self_attn.o_proj_mean': array(-0.01531, dtype=float16), 'model.layers.5.self_attn.o_proj_sigma': array(1.595, dtype=float16), 'model.layers.5.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.5.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 5.881022661924362e-05, 'model.layers.5.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 73.0479294709987, 'model.layers.5.self_attn.s_quantizer': 0.0, 'model.layers.5.self_attn.s_quantizer_mean': -0.02729654312133789, 'model.layers.5.self_attn.s_quantizer_sigma': 53.665631459994955, 'model.layers.5.mlp.gate_proj': 0.0, 'model.layers.5.mlp.gate_proj_mean': array(0.005337, dtype=float16), 'model.layers.5.mlp.gate_proj_sigma': array(2.748, dtype=float16), 'model.layers.5.mlp.up_proj': 0.0, 'model.layers.5.mlp.up_proj_mean': array(0.005337, dtype=float16), 'model.layers.5.mlp.up_proj_sigma': array(2.748, dtype=float16), 'model.layers.5.mlp.down_proj': 0.0, 'model.layers.5.mlp.down_proj_mean': array(0.001668, dtype=float16), 'model.layers.5.mlp.down_proj_sigma': array(2.723, dtype=float16)}, {'model.layers.6.self_attn.q_proj': 0.0, 'model.layers.6.self_attn.q_proj_mean': array(-0.00883, dtype=float16), 'model.layers.6.self_attn.q_proj_sigma': array(2.66, dtype=float16), 'model.layers.6.self_attn.k_proj': 0.0, 'model.layers.6.self_attn.k_proj_mean': array(-0.00883, dtype=float16), 'model.layers.6.self_attn.k_proj_sigma': array(2.66, dtype=float16), 'model.layers.6.self_attn.v_proj': 0.0, 'model.layers.6.self_attn.v_proj_mean': array(-0.00883, dtype=float16), 'model.layers.6.self_attn.v_proj_sigma': array(2.66, dtype=float16), 'model.layers.6.self_attn.o_proj': 0.0, 'model.layers.6.self_attn.o_proj_mean': array(-0.03192, dtype=float16), 'model.layers.6.self_attn.o_proj_sigma': array(1.865, dtype=float16), 'model.layers.6.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.6.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -5.497236270457506e-05, 'model.layers.6.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 72.49827584156743, 'model.layers.6.self_attn.s_quantizer': 0.0, 'model.layers.6.self_attn.s_quantizer_mean': -0.1086696982383728, 'model.layers.6.self_attn.s_quantizer_sigma': 50.872389367907616, 'model.layers.6.mlp.gate_proj': 0.0, 'model.layers.6.mlp.gate_proj_mean': array(-0.001441, dtype=float16), 'model.layers.6.mlp.gate_proj_sigma': array(2.715, dtype=float16), 'model.layers.6.mlp.up_proj': 0.0, 'model.layers.6.mlp.up_proj_mean': array(-0.001441, dtype=float16), 'model.layers.6.mlp.up_proj_sigma': array(2.715, dtype=float16), 'model.layers.6.mlp.down_proj': 0.0, 'model.layers.6.mlp.down_proj_mean': array(-0.00431, dtype=float16), 'model.layers.6.mlp.down_proj_sigma': array(2.688, dtype=float16)}, {'model.layers.7.self_attn.q_proj': 0.0, 'model.layers.7.self_attn.q_proj_mean': array(-0.004177, dtype=float16), 'model.layers.7.self_attn.q_proj_sigma': array(2.76, dtype=float16), 'model.layers.7.self_attn.k_proj': 0.0, 'model.layers.7.self_attn.k_proj_mean': array(-0.004177, dtype=float16), 'model.layers.7.self_attn.k_proj_sigma': array(2.76, dtype=float16), 'model.layers.7.self_attn.v_proj': 0.0, 'model.layers.7.self_attn.v_proj_mean': array(-0.004177, dtype=float16), 'model.layers.7.self_attn.v_proj_sigma': array(2.76, dtype=float16), 'model.layers.7.self_attn.o_proj': 0.0, 'model.layers.7.self_attn.o_proj_mean': array(-0.00471, dtype=float16), 'model.layers.7.self_attn.o_proj_sigma': array(2.12, dtype=float16), 'model.layers.7.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.7.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.00020937074441462755, 'model.layers.7.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 76.68115805072325, 'model.layers.7.self_attn.s_quantizer': 0.0, 'model.layers.7.self_attn.s_quantizer_mean': -0.07645797729492188, 'model.layers.7.self_attn.s_quantizer_sigma': 51.18593556827891, 'model.layers.7.mlp.gate_proj': 0.0, 'model.layers.7.mlp.gate_proj_mean': array(0.005566, dtype=float16), 'model.layers.7.mlp.gate_proj_sigma': array(2.777, dtype=float16), 'model.layers.7.mlp.up_proj': 0.0, 'model.layers.7.mlp.up_proj_mean': array(0.005566, dtype=float16), 'model.layers.7.mlp.up_proj_sigma': array(2.777, dtype=float16), 'model.layers.7.mlp.down_proj': 0.0, 'model.layers.7.mlp.down_proj_mean': array(0.001169, dtype=float16), 'model.layers.7.mlp.down_proj_sigma': array(2.684, dtype=float16)}, {'model.layers.8.self_attn.q_proj': 0.0, 'model.layers.8.self_attn.q_proj_mean': array(-0.005116, dtype=float16), 'model.layers.8.self_attn.q_proj_sigma': array(2.787, dtype=float16), 'model.layers.8.self_attn.k_proj': 0.0, 'model.layers.8.self_attn.k_proj_mean': array(-0.005116, dtype=float16), 'model.layers.8.self_attn.k_proj_sigma': array(2.787, dtype=float16), 'model.layers.8.self_attn.v_proj': 0.0, 'model.layers.8.self_attn.v_proj_mean': array(-0.005116, dtype=float16), 'model.layers.8.self_attn.v_proj_sigma': array(2.787, dtype=float16), 'model.layers.8.self_attn.o_proj': 0.0, 'model.layers.8.self_attn.o_proj_mean': array(-0.01482, dtype=float16), 'model.layers.8.self_attn.o_proj_sigma': array(2.084, dtype=float16), 'model.layers.8.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.8.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 3.347313031554222e-05, 'model.layers.8.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 73.0479294709987, 'model.layers.8.self_attn.s_quantizer': 0.0, 'model.layers.8.self_attn.s_quantizer_mean': 0.09287309646606445, 'model.layers.8.self_attn.s_quantizer_sigma': 54.11099703387473, 'model.layers.8.mlp.gate_proj': 0.0, 'model.layers.8.mlp.gate_proj_mean': array(-0.000833, dtype=float16), 'model.layers.8.mlp.gate_proj_sigma': array(2.803, dtype=float16), 'model.layers.8.mlp.up_proj': 0.0, 'model.layers.8.mlp.up_proj_mean': array(-0.000833, dtype=float16), 'model.layers.8.mlp.up_proj_sigma': array(2.803, dtype=float16), 'model.layers.8.mlp.down_proj': 0.0, 'model.layers.8.mlp.down_proj_mean': array(-0.002317, dtype=float16), 'model.layers.8.mlp.down_proj_sigma': array(2.865, dtype=float16)}, {'model.layers.9.self_attn.q_proj': 0.0, 'model.layers.9.self_attn.q_proj_mean': array(0.0001618, dtype=float16), 'model.layers.9.self_attn.q_proj_sigma': array(2.588, dtype=float16), 'model.layers.9.self_attn.k_proj': 0.0, 'model.layers.9.self_attn.k_proj_mean': array(0.0001618, dtype=float16), 'model.layers.9.self_attn.k_proj_sigma': array(2.588, dtype=float16), 'model.layers.9.self_attn.v_proj': 0.0, 'model.layers.9.self_attn.v_proj_mean': array(0.0001618, dtype=float16), 'model.layers.9.self_attn.v_proj_sigma': array(2.588, dtype=float16), 'model.layers.9.self_attn.o_proj': 0.0, 'model.layers.9.self_attn.o_proj_mean': array(-0.01537, dtype=float16), 'model.layers.9.self_attn.o_proj_sigma': array(2.21, dtype=float16), 'model.layers.9.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.9.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 6.828689947724342e-05, 'model.layers.9.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 75.49834435270749, 'model.layers.9.self_attn.s_quantizer': 0.0, 'model.layers.9.self_attn.s_quantizer_mean': -0.2960028648376465, 'model.layers.9.self_attn.s_quantizer_sigma': 53.55371135598354, 'model.layers.9.mlp.gate_proj': 0.0, 'model.layers.9.mlp.gate_proj_mean': array(-0.004063, dtype=float16), 'model.layers.9.mlp.gate_proj_sigma': array(2.787, dtype=float16), 'model.layers.9.mlp.up_proj': 0.0, 'model.layers.9.mlp.up_proj_mean': array(-0.004063, dtype=float16), 'model.layers.9.mlp.up_proj_sigma': array(2.787, dtype=float16), 'model.layers.9.mlp.down_proj': 0.0, 'model.layers.9.mlp.down_proj_mean': array(-0.007458, dtype=float16), 'model.layers.9.mlp.down_proj_sigma': array(2.867, dtype=float16)}, {'model.layers.10.self_attn.q_proj': 0.0, 'model.layers.10.self_attn.q_proj_mean': array(-0.001731, dtype=float16), 'model.layers.10.self_attn.q_proj_sigma': array(2.424, dtype=float16), 'model.layers.10.self_attn.k_proj': 0.0, 'model.layers.10.self_attn.k_proj_mean': array(-0.001731, dtype=float16), 'model.layers.10.self_attn.k_proj_sigma': array(2.424, dtype=float16), 'model.layers.10.self_attn.v_proj': 0.0, 'model.layers.10.self_attn.v_proj_mean': array(-0.001731, dtype=float16), 'model.layers.10.self_attn.v_proj_sigma': array(2.424, dtype=float16), 'model.layers.10.self_attn.o_proj': 0.0, 'model.layers.10.self_attn.o_proj_mean': array(0.00785, dtype=float16), 'model.layers.10.self_attn.o_proj_sigma': array(2.234, dtype=float16), 'model.layers.10.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.10.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -1.1259689927101135e-05, 'model.layers.10.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 74.4043009509531, 'model.layers.10.self_attn.s_quantizer': 0.0, 'model.layers.10.self_attn.s_quantizer_mean': 0.2801358699798584, 'model.layers.10.self_attn.s_quantizer_sigma': 52.64978632435273, 'model.layers.10.mlp.gate_proj': 0.0, 'model.layers.10.mlp.gate_proj_mean': array(-0.001227, dtype=float16), 'model.layers.10.mlp.gate_proj_sigma': array(2.709, dtype=float16), 'model.layers.10.mlp.up_proj': 0.0, 'model.layers.10.mlp.up_proj_mean': array(-0.001227, dtype=float16), 'model.layers.10.mlp.up_proj_sigma': array(2.709, dtype=float16), 'model.layers.10.mlp.down_proj': 0.0, 'model.layers.10.mlp.down_proj_mean': array(-0.006474, dtype=float16), 'model.layers.10.mlp.down_proj_sigma': array(2.916, dtype=float16)}, {'model.layers.11.self_attn.q_proj': 0.0, 'model.layers.11.self_attn.q_proj_mean': array(0.0003107, dtype=float16), 'model.layers.11.self_attn.q_proj_sigma': array(2.705, dtype=float16), 'model.layers.11.self_attn.k_proj': 0.0, 'model.layers.11.self_attn.k_proj_mean': array(0.0003107, dtype=float16), 'model.layers.11.self_attn.k_proj_sigma': array(2.705, dtype=float16), 'model.layers.11.self_attn.v_proj': 0.0, 'model.layers.11.self_attn.v_proj_mean': array(0.0003107, dtype=float16), 'model.layers.11.self_attn.v_proj_sigma': array(2.705, dtype=float16), 'model.layers.11.self_attn.o_proj': 0.0, 'model.layers.11.self_attn.o_proj_mean': array(-0.04254, dtype=float16), 'model.layers.11.self_attn.o_proj_sigma': array(2.115, dtype=float16), 'model.layers.11.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.11.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.0001427559182047844, 'model.layers.11.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 74.08103670980854, 'model.layers.11.self_attn.s_quantizer': 0.0, 'model.layers.11.self_attn.s_quantizer_mean': -0.48339056968688965, 'model.layers.11.self_attn.s_quantizer_sigma': 57.67148342118486, 'model.layers.11.mlp.gate_proj': 0.0, 'model.layers.11.mlp.gate_proj_mean': array(-4.97e-05, dtype=float16), 'model.layers.11.mlp.gate_proj_sigma': array(2.717, dtype=float16), 'model.layers.11.mlp.up_proj': 0.0, 'model.layers.11.mlp.up_proj_mean': array(-4.97e-05, dtype=float16), 'model.layers.11.mlp.up_proj_sigma': array(2.717, dtype=float16), 'model.layers.11.mlp.down_proj': 0.0, 'model.layers.11.mlp.down_proj_mean': array(-0.005444, dtype=float16), 'model.layers.11.mlp.down_proj_sigma': array(2.832, dtype=float16)}, {'model.layers.12.self_attn.q_proj': 0.0, 'model.layers.12.self_attn.q_proj_mean': array(0.005016, dtype=float16), 'model.layers.12.self_attn.q_proj_sigma': array(2.732, dtype=float16), 'model.layers.12.self_attn.k_proj': 0.0, 'model.layers.12.self_attn.k_proj_mean': array(0.005016, dtype=float16), 'model.layers.12.self_attn.k_proj_sigma': array(2.732, dtype=float16), 'model.layers.12.self_attn.v_proj': 0.0, 'model.layers.12.self_attn.v_proj_mean': array(0.005016, dtype=float16), 'model.layers.12.self_attn.v_proj_sigma': array(2.732, dtype=float16), 'model.layers.12.self_attn.o_proj': 0.0, 'model.layers.12.self_attn.o_proj_mean': array(-0.01578, dtype=float16), 'model.layers.12.self_attn.o_proj_sigma': array(2.12, dtype=float16), 'model.layers.12.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.12.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.00012098869774490595, 'model.layers.12.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 77.87168933572714, 'model.layers.12.self_attn.s_quantizer': 0.0, 'model.layers.12.self_attn.s_quantizer_mean': -0.19974684715270996, 'model.layers.12.self_attn.s_quantizer_sigma': 64.15605972938177, 'model.layers.12.mlp.gate_proj': 0.0, 'model.layers.12.mlp.gate_proj_mean': array(0.007042, dtype=float16), 'model.layers.12.mlp.gate_proj_sigma': array(2.742, dtype=float16), 'model.layers.12.mlp.up_proj': 0.0, 'model.layers.12.mlp.up_proj_mean': array(0.007042, dtype=float16), 'model.layers.12.mlp.up_proj_sigma': array(2.742, dtype=float16), 'model.layers.12.mlp.down_proj': 0.0, 'model.layers.12.mlp.down_proj_mean': array(-0.003517, dtype=float16), 'model.layers.12.mlp.down_proj_sigma': array(2.865, dtype=float16)}, {'model.layers.13.self_attn.q_proj': 0.0, 'model.layers.13.self_attn.q_proj_mean': array(0.01662, dtype=float16), 'model.layers.13.self_attn.q_proj_sigma': array(2.78, dtype=float16), 'model.layers.13.self_attn.k_proj': 0.0, 'model.layers.13.self_attn.k_proj_mean': array(0.01662, dtype=float16), 'model.layers.13.self_attn.k_proj_sigma': array(2.78, dtype=float16), 'model.layers.13.self_attn.v_proj': 0.0, 'model.layers.13.self_attn.v_proj_mean': array(0.01662, dtype=float16), 'model.layers.13.self_attn.v_proj_sigma': array(2.78, dtype=float16), 'model.layers.13.self_attn.o_proj': 0.0, 'model.layers.13.self_attn.o_proj_mean': array(-0.02406, dtype=float16), 'model.layers.13.self_attn.o_proj_sigma': array(2.295, dtype=float16), 'model.layers.13.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.13.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -4.007597453892231e-06, 'model.layers.13.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 76.15773105863909, 'model.layers.13.self_attn.s_quantizer': 0.0, 'model.layers.13.self_attn.s_quantizer_mean': -0.3801884651184082, 'model.layers.13.self_attn.s_quantizer_sigma': 53.98147830506312, 'model.layers.13.mlp.gate_proj': 0.0, 'model.layers.13.mlp.gate_proj_mean': array(0.00921, dtype=float16), 'model.layers.13.mlp.gate_proj_sigma': array(2.836, dtype=float16), 'model.layers.13.mlp.up_proj': 0.0, 'model.layers.13.mlp.up_proj_mean': array(0.00921, dtype=float16), 'model.layers.13.mlp.up_proj_sigma': array(2.836, dtype=float16), 'model.layers.13.mlp.down_proj': 0.0, 'model.layers.13.mlp.down_proj_mean': array(-0.000982, dtype=float16), 'model.layers.13.mlp.down_proj_sigma': array(2.69, dtype=float16)}, {'model.layers.14.self_attn.q_proj': 0.0, 'model.layers.14.self_attn.q_proj_mean': array(0.01819, dtype=float16), 'model.layers.14.self_attn.q_proj_sigma': array(2.658, dtype=float16), 'model.layers.14.self_attn.k_proj': 0.0, 'model.layers.14.self_attn.k_proj_mean': array(0.01819, dtype=float16), 'model.layers.14.self_attn.k_proj_sigma': array(2.658, dtype=float16), 'model.layers.14.self_attn.v_proj': 0.0, 'model.layers.14.self_attn.v_proj_mean': array(0.01819, dtype=float16), 'model.layers.14.self_attn.v_proj_sigma': array(2.658, dtype=float16), 'model.layers.14.self_attn.o_proj': 0.0, 'model.layers.14.self_attn.o_proj_mean': array(-0.01283, dtype=float16), 'model.layers.14.self_attn.o_proj_sigma': array(1.882, dtype=float16), 'model.layers.14.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.14.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -2.99729872494936e-05, 'model.layers.14.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 77.30459236035075, 'model.layers.14.self_attn.s_quantizer': 0.0, 'model.layers.14.self_attn.s_quantizer_mean': -0.11583542823791504, 'model.layers.14.self_attn.s_quantizer_sigma': 51.419840528729765, 'model.layers.14.mlp.gate_proj': 0.0, 'model.layers.14.mlp.gate_proj_mean': array(0.01379, dtype=float16), 'model.layers.14.mlp.gate_proj_sigma': array(2.783, dtype=float16), 'model.layers.14.mlp.up_proj': 0.0, 'model.layers.14.mlp.up_proj_mean': array(0.01379, dtype=float16), 'model.layers.14.mlp.up_proj_sigma': array(2.783, dtype=float16), 'model.layers.14.mlp.down_proj': 0.0, 'model.layers.14.mlp.down_proj_mean': array(-0.00339, dtype=float16), 'model.layers.14.mlp.down_proj_sigma': array(2.865, dtype=float16)}, {'model.layers.15.self_attn.q_proj': 0.0, 'model.layers.15.self_attn.q_proj_mean': array(0.007484, dtype=float16), 'model.layers.15.self_attn.q_proj_sigma': array(2.688, dtype=float16), 'model.layers.15.self_attn.k_proj': 0.0, 'model.layers.15.self_attn.k_proj_mean': array(0.007484, dtype=float16), 'model.layers.15.self_attn.k_proj_sigma': array(2.688, dtype=float16), 'model.layers.15.self_attn.v_proj': 0.0, 'model.layers.15.self_attn.v_proj_mean': array(0.007484, dtype=float16), 'model.layers.15.self_attn.v_proj_sigma': array(2.688, dtype=float16), 'model.layers.15.self_attn.o_proj': 0.0, 'model.layers.15.self_attn.o_proj_mean': array(-0.04062, dtype=float16), 'model.layers.15.self_attn.o_proj_sigma': array(2.246, dtype=float16), 'model.layers.15.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.15.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -4.550290759652853e-05, 'model.layers.15.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 73.86474125047755, 'model.layers.15.self_attn.s_quantizer': 0.0, 'model.layers.15.self_attn.s_quantizer_mean': -0.3441433906555176, 'model.layers.15.self_attn.s_quantizer_sigma': 48.31148931672465, 'model.layers.15.mlp.gate_proj': 0.0, 'model.layers.15.mlp.gate_proj_mean': array(0.012474, dtype=float16), 'model.layers.15.mlp.gate_proj_sigma': array(2.66, dtype=float16), 'model.layers.15.mlp.up_proj': 0.0, 'model.layers.15.mlp.up_proj_mean': array(0.012474, dtype=float16), 'model.layers.15.mlp.up_proj_sigma': array(2.66, dtype=float16), 'model.layers.15.mlp.down_proj': 0.0, 'model.layers.15.mlp.down_proj_mean': array(-0.00464, dtype=float16), 'model.layers.15.mlp.down_proj_sigma': array(2.883, dtype=float16)}, {'model.layers.16.self_attn.q_proj': 0.0, 'model.layers.16.self_attn.q_proj_mean': array(0.01316, dtype=float16), 'model.layers.16.self_attn.q_proj_sigma': array(2.748, dtype=float16), 'model.layers.16.self_attn.k_proj': 0.0, 'model.layers.16.self_attn.k_proj_mean': array(0.01316, dtype=float16), 'model.layers.16.self_attn.k_proj_sigma': array(2.748, dtype=float16), 'model.layers.16.self_attn.v_proj': 0.0, 'model.layers.16.self_attn.v_proj_mean': array(0.01316, dtype=float16), 'model.layers.16.self_attn.v_proj_sigma': array(2.748, dtype=float16), 'model.layers.16.self_attn.o_proj': 0.0, 'model.layers.16.self_attn.o_proj_mean': array(-0.003386, dtype=float16), 'model.layers.16.self_attn.o_proj_sigma': array(2.113, dtype=float16), 'model.layers.16.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.16.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 1.2286007404327393e-05, 'model.layers.16.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 77.33045971672482, 'model.layers.16.self_attn.s_quantizer': 0.0, 'model.layers.16.self_attn.s_quantizer_mean': -0.16588711738586426, 'model.layers.16.self_attn.s_quantizer_sigma': 48.16637831516918, 'model.layers.16.mlp.gate_proj': 0.0, 'model.layers.16.mlp.gate_proj_mean': array(0.002869, dtype=float16), 'model.layers.16.mlp.gate_proj_sigma': array(2.668, dtype=float16), 'model.layers.16.mlp.up_proj': 0.0, 'model.layers.16.mlp.up_proj_mean': array(0.002869, dtype=float16), 'model.layers.16.mlp.up_proj_sigma': array(2.668, dtype=float16), 'model.layers.16.mlp.down_proj': 0.0, 'model.layers.16.mlp.down_proj_mean': array(-0.003809, dtype=float16), 'model.layers.16.mlp.down_proj_sigma': array(2.84, dtype=float16)}, {'model.layers.17.self_attn.q_proj': 0.0, 'model.layers.17.self_attn.q_proj_mean': array(0.0006847, dtype=float16), 'model.layers.17.self_attn.q_proj_sigma': array(2.64, dtype=float16), 'model.layers.17.self_attn.k_proj': 0.0, 'model.layers.17.self_attn.k_proj_mean': array(0.0006847, dtype=float16), 'model.layers.17.self_attn.k_proj_sigma': array(2.64, dtype=float16), 'model.layers.17.self_attn.v_proj': 0.0, 'model.layers.17.self_attn.v_proj_mean': array(0.0006847, dtype=float16), 'model.layers.17.self_attn.v_proj_sigma': array(2.64, dtype=float16), 'model.layers.17.self_attn.o_proj': 0.0, 'model.layers.17.self_attn.o_proj_mean': array(0.01904, dtype=float16), 'model.layers.17.self_attn.o_proj_sigma': array(1.565, dtype=float16), 'model.layers.17.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.17.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 7.673032814636827e-05, 'model.layers.17.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 79.27168473042566, 'model.layers.17.self_attn.s_quantizer': 0.0, 'model.layers.17.self_attn.s_quantizer_mean': 0.23757219314575195, 'model.layers.17.self_attn.s_quantizer_sigma': 47.11687595755899, 'model.layers.17.mlp.gate_proj': 0.0, 'model.layers.17.mlp.gate_proj_mean': array(-0.00222, dtype=float16), 'model.layers.17.mlp.gate_proj_sigma': array(2.723, dtype=float16), 'model.layers.17.mlp.up_proj': 0.0, 'model.layers.17.mlp.up_proj_mean': array(-0.00222, dtype=float16), 'model.layers.17.mlp.up_proj_sigma': array(2.723, dtype=float16), 'model.layers.17.mlp.down_proj': 0.0, 'model.layers.17.mlp.down_proj_mean': array(0.005157, dtype=float16), 'model.layers.17.mlp.down_proj_sigma': array(2.832, dtype=float16)}, {'model.layers.18.self_attn.q_proj': 0.0, 'model.layers.18.self_attn.q_proj_mean': array(0.0006533, dtype=float16), 'model.layers.18.self_attn.q_proj_sigma': array(2.43, dtype=float16), 'model.layers.18.self_attn.k_proj': 0.0, 'model.layers.18.self_attn.k_proj_mean': array(0.0006533, dtype=float16), 'model.layers.18.self_attn.k_proj_sigma': array(2.43, dtype=float16), 'model.layers.18.self_attn.v_proj': 0.0, 'model.layers.18.self_attn.v_proj_mean': array(0.0006533, dtype=float16), 'model.layers.18.self_attn.v_proj_sigma': array(2.43, dtype=float16), 'model.layers.18.self_attn.o_proj': 0.0, 'model.layers.18.self_attn.o_proj_mean': array(-0.00392, dtype=float16), 'model.layers.18.self_attn.o_proj_sigma': array(1.85, dtype=float16), 'model.layers.18.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.18.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 3.861438017338514e-05, 'model.layers.18.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 76.68115805072325, 'model.layers.18.self_attn.s_quantizer': 0.0, 'model.layers.18.self_attn.s_quantizer_mean': 0.0995287299156189, 'model.layers.18.self_attn.s_quantizer_sigma': 43.12771730569565, 'model.layers.18.mlp.gate_proj': 0.0, 'model.layers.18.mlp.gate_proj_mean': array(-0.002605, dtype=float16), 'model.layers.18.mlp.gate_proj_sigma': array(2.738, dtype=float16), 'model.layers.18.mlp.up_proj': 0.0, 'model.layers.18.mlp.up_proj_mean': array(-0.002605, dtype=float16), 'model.layers.18.mlp.up_proj_sigma': array(2.738, dtype=float16), 'model.layers.18.mlp.down_proj': 0.0, 'model.layers.18.mlp.down_proj_mean': array(-0.002844, dtype=float16), 'model.layers.18.mlp.down_proj_sigma': array(2.764, dtype=float16)}, {'model.layers.19.self_attn.q_proj': 0.0, 'model.layers.19.self_attn.q_proj_mean': array(0.001646, dtype=float16), 'model.layers.19.self_attn.q_proj_sigma': array(2.592, dtype=float16), 'model.layers.19.self_attn.k_proj': 0.0, 'model.layers.19.self_attn.k_proj_mean': array(0.001646, dtype=float16), 'model.layers.19.self_attn.k_proj_sigma': array(2.592, dtype=float16), 'model.layers.19.self_attn.v_proj': 0.0, 'model.layers.19.self_attn.v_proj_mean': array(0.001646, dtype=float16), 'model.layers.19.self_attn.v_proj_sigma': array(2.592, dtype=float16), 'model.layers.19.self_attn.o_proj': 0.0, 'model.layers.19.self_attn.o_proj_mean': array(0.005783, dtype=float16), 'model.layers.19.self_attn.o_proj_sigma': array(1.49, dtype=float16), 'model.layers.19.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.19.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 4.8078305553644896e-05, 'model.layers.19.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 74.4043009509531, 'model.layers.19.self_attn.s_quantizer': 0.0, 'model.layers.19.self_attn.s_quantizer_mean': 0.2630615234375, 'model.layers.19.self_attn.s_quantizer_sigma': 44.54211490264017, 'model.layers.19.mlp.gate_proj': 0.0, 'model.layers.19.mlp.gate_proj_mean': array(-0.00501, dtype=float16), 'model.layers.19.mlp.gate_proj_sigma': array(3.002, dtype=float16), 'model.layers.19.mlp.up_proj': 0.0, 'model.layers.19.mlp.up_proj_mean': array(-0.00501, dtype=float16), 'model.layers.19.mlp.up_proj_sigma': array(3.002, dtype=float16), 'model.layers.19.mlp.down_proj': 0.0, 'model.layers.19.mlp.down_proj_mean': array(0.000576, dtype=float16), 'model.layers.19.mlp.down_proj_sigma': array(2.488, dtype=float16)}, {'model.layers.20.self_attn.q_proj': 0.0, 'model.layers.20.self_attn.q_proj_mean': array(-0.001886, dtype=float16), 'model.layers.20.self_attn.q_proj_sigma': array(2.512, dtype=float16), 'model.layers.20.self_attn.k_proj': 0.0, 'model.layers.20.self_attn.k_proj_mean': array(-0.001886, dtype=float16), 'model.layers.20.self_attn.k_proj_sigma': array(2.512, dtype=float16), 'model.layers.20.self_attn.v_proj': 0.0, 'model.layers.20.self_attn.v_proj_mean': array(-0.001886, dtype=float16), 'model.layers.20.self_attn.v_proj_sigma': array(2.512, dtype=float16), 'model.layers.20.self_attn.o_proj': 0.0, 'model.layers.20.self_attn.o_proj_mean': array(-0.005917, dtype=float16), 'model.layers.20.self_attn.o_proj_sigma': array(1.471, dtype=float16), 'model.layers.20.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.20.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.00011362135410308838, 'model.layers.20.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 71.02112361825881, 'model.layers.20.self_attn.s_quantizer': 0.0, 'model.layers.20.self_attn.s_quantizer_mean': 0.04613816738128662, 'model.layers.20.self_attn.s_quantizer_sigma': 40.24922359499622, 'model.layers.20.mlp.gate_proj': 0.0, 'model.layers.20.mlp.gate_proj_mean': array(-0.002766, dtype=float16), 'model.layers.20.mlp.gate_proj_sigma': array(2.883, dtype=float16), 'model.layers.20.mlp.up_proj': 0.0, 'model.layers.20.mlp.up_proj_mean': array(-0.002766, dtype=float16), 'model.layers.20.mlp.up_proj_sigma': array(2.883, dtype=float16), 'model.layers.20.mlp.down_proj': 0.0, 'model.layers.20.mlp.down_proj_mean': array(-0.006157, dtype=float16), 'model.layers.20.mlp.down_proj_sigma': array(2.484, dtype=float16)}, {'model.layers.21.self_attn.q_proj': 0.0, 'model.layers.21.self_attn.q_proj_mean': array(6.9e-06, dtype=float16), 'model.layers.21.self_attn.q_proj_sigma': array(2.562, dtype=float16), 'model.layers.21.self_attn.k_proj': 0.0, 'model.layers.21.self_attn.k_proj_mean': array(6.9e-06, dtype=float16), 'model.layers.21.self_attn.k_proj_sigma': array(2.562, dtype=float16), 'model.layers.21.self_attn.v_proj': 0.0, 'model.layers.21.self_attn.v_proj_mean': array(6.9e-06, dtype=float16), 'model.layers.21.self_attn.v_proj_sigma': array(2.562, dtype=float16), 'model.layers.21.self_attn.o_proj': 0.0, 'model.layers.21.self_attn.o_proj_mean': array(-0.01344, dtype=float16), 'model.layers.21.self_attn.o_proj_sigma': array(1.476, dtype=float16), 'model.layers.21.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.21.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 4.5071239583194256e-05, 'model.layers.21.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 77.64019577512668, 'model.layers.21.self_attn.s_quantizer': 0.0, 'model.layers.21.self_attn.s_quantizer_mean': -0.27678680419921875, 'model.layers.21.self_attn.s_quantizer_sigma': 45.51922670696417, 'model.layers.21.mlp.gate_proj': 0.0, 'model.layers.21.mlp.gate_proj_mean': array(-0.003578, dtype=float16), 'model.layers.21.mlp.gate_proj_sigma': array(3.025, dtype=float16), 'model.layers.21.mlp.up_proj': 0.0, 'model.layers.21.mlp.up_proj_mean': array(-0.003578, dtype=float16), 'model.layers.21.mlp.up_proj_sigma': array(3.025, dtype=float16), 'model.layers.21.mlp.down_proj': 0.0, 'model.layers.21.mlp.down_proj_mean': array(0.000164, dtype=float16), 'model.layers.21.mlp.down_proj_sigma': array(2.4, dtype=float16)}, {'model.layers.22.self_attn.q_proj': 0.0, 'model.layers.22.self_attn.q_proj_mean': array(-0.000928, dtype=float16), 'model.layers.22.self_attn.q_proj_sigma': array(2.635, dtype=float16), 'model.layers.22.self_attn.k_proj': 0.0, 'model.layers.22.self_attn.k_proj_mean': array(-0.000928, dtype=float16), 'model.layers.22.self_attn.k_proj_sigma': array(2.635, dtype=float16), 'model.layers.22.self_attn.v_proj': 0.0, 'model.layers.22.self_attn.v_proj_mean': array(-0.000928, dtype=float16), 'model.layers.22.self_attn.v_proj_sigma': array(2.635, dtype=float16), 'model.layers.22.self_attn.o_proj': 0.0, 'model.layers.22.self_attn.o_proj_mean': array(-0.01152, dtype=float16), 'model.layers.22.self_attn.o_proj_sigma': array(1.437, dtype=float16), 'model.layers.22.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.22.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 1.4087534509599209e-05, 'model.layers.22.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 75.31268153505093, 'model.layers.22.self_attn.s_quantizer': 0.0, 'model.layers.22.self_attn.s_quantizer_mean': -0.016038671135902405, 'model.layers.22.self_attn.s_quantizer_sigma': 36.9729630946723, 'model.layers.22.mlp.gate_proj': 0.0, 'model.layers.22.mlp.gate_proj_mean': array(-0.005184, dtype=float16), 'model.layers.22.mlp.gate_proj_sigma': array(3.03, dtype=float16), 'model.layers.22.mlp.up_proj': 0.0, 'model.layers.22.mlp.up_proj_mean': array(-0.005184, dtype=float16), 'model.layers.22.mlp.up_proj_sigma': array(3.03, dtype=float16), 'model.layers.22.mlp.down_proj': 0.0, 'model.layers.22.mlp.down_proj_mean': array(-0.002785, dtype=float16), 'model.layers.22.mlp.down_proj_sigma': array(2.498, dtype=float16)}, {'model.layers.23.self_attn.q_proj': 0.0, 'model.layers.23.self_attn.q_proj_mean': array(-0.002745, dtype=float16), 'model.layers.23.self_attn.q_proj_sigma': array(2.602, dtype=float16), 'model.layers.23.self_attn.k_proj': 0.0, 'model.layers.23.self_attn.k_proj_mean': array(-0.002745, dtype=float16), 'model.layers.23.self_attn.k_proj_sigma': array(2.602, dtype=float16), 'model.layers.23.self_attn.v_proj': 0.0, 'model.layers.23.self_attn.v_proj_mean': array(-0.002745, dtype=float16), 'model.layers.23.self_attn.v_proj_sigma': array(2.602, dtype=float16), 'model.layers.23.self_attn.o_proj': 0.0, 'model.layers.23.self_attn.o_proj_mean': array(0.0008826, dtype=float16), 'model.layers.23.self_attn.o_proj_sigma': array(1.109, dtype=float16), 'model.layers.23.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.23.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -0.00017794460291042924, 'model.layers.23.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 78.86697661252141, 'model.layers.23.self_attn.s_quantizer': 0.0, 'model.layers.23.self_attn.s_quantizer_mean': -0.06141588091850281, 'model.layers.23.self_attn.s_quantizer_sigma': 37.33630940518894, 'model.layers.23.mlp.gate_proj': 0.0, 'model.layers.23.mlp.gate_proj_mean': array(-0.010155, dtype=float16), 'model.layers.23.mlp.gate_proj_sigma': array(2.994, dtype=float16), 'model.layers.23.mlp.up_proj': 0.0, 'model.layers.23.mlp.up_proj_mean': array(-0.010155, dtype=float16), 'model.layers.23.mlp.up_proj_sigma': array(2.994, dtype=float16), 'model.layers.23.mlp.down_proj': 0.0, 'model.layers.23.mlp.down_proj_mean': array(-0.0015135, dtype=float16), 'model.layers.23.mlp.down_proj_sigma': array(2.453, dtype=float16)}, {'model.layers.24.self_attn.q_proj': 0.0, 'model.layers.24.self_attn.q_proj_mean': array(-0.005394, dtype=float16), 'model.layers.24.self_attn.q_proj_sigma': array(2.576, dtype=float16), 'model.layers.24.self_attn.k_proj': 0.0, 'model.layers.24.self_attn.k_proj_mean': array(-0.005394, dtype=float16), 'model.layers.24.self_attn.k_proj_sigma': array(2.576, dtype=float16), 'model.layers.24.self_attn.v_proj': 0.0, 'model.layers.24.self_attn.v_proj_mean': array(-0.005394, dtype=float16), 'model.layers.24.self_attn.v_proj_sigma': array(2.576, dtype=float16), 'model.layers.24.self_attn.o_proj': 0.0, 'model.layers.24.self_attn.o_proj_mean': array(0.00669, dtype=float16), 'model.layers.24.self_attn.o_proj_sigma': array(1.226, dtype=float16), 'model.layers.24.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.24.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.00027447869069874287, 'model.layers.24.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 78.20485918406861, 'model.layers.24.self_attn.s_quantizer': 0.0, 'model.layers.24.self_attn.s_quantizer_mean': -0.13387668132781982, 'model.layers.24.self_attn.s_quantizer_sigma': 34.55430508634199, 'model.layers.24.mlp.gate_proj': 0.0, 'model.layers.24.mlp.gate_proj_mean': array(-0.007095, dtype=float16), 'model.layers.24.mlp.gate_proj_sigma': array(2.654, dtype=float16), 'model.layers.24.mlp.up_proj': 0.0, 'model.layers.24.mlp.up_proj_mean': array(-0.007095, dtype=float16), 'model.layers.24.mlp.up_proj_sigma': array(2.654, dtype=float16), 'model.layers.24.mlp.down_proj': 0.0, 'model.layers.24.mlp.down_proj_mean': array(-0.01065, dtype=float16), 'model.layers.24.mlp.down_proj_sigma': array(2.838, dtype=float16)}, {'model.layers.25.self_attn.q_proj': 0.0, 'model.layers.25.self_attn.q_proj_mean': array(-0.004642, dtype=float16), 'model.layers.25.self_attn.q_proj_sigma': array(2.701, dtype=float16), 'model.layers.25.self_attn.k_proj': 0.0, 'model.layers.25.self_attn.k_proj_mean': array(-0.004642, dtype=float16), 'model.layers.25.self_attn.k_proj_sigma': array(2.701, dtype=float16), 'model.layers.25.self_attn.v_proj': 0.0, 'model.layers.25.self_attn.v_proj_mean': array(-0.004642, dtype=float16), 'model.layers.25.self_attn.v_proj_sigma': array(2.701, dtype=float16), 'model.layers.25.self_attn.o_proj': 0.0, 'model.layers.25.self_attn.o_proj_mean': array(-0.02814, dtype=float16), 'model.layers.25.self_attn.o_proj_sigma': array(1.374, dtype=float16), 'model.layers.25.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.25.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 5.510915070772171e-05, 'model.layers.25.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 78.63841300535, 'model.layers.25.self_attn.s_quantizer': 0.0, 'model.layers.25.self_attn.s_quantizer_mean': -0.09506607055664062, 'model.layers.25.self_attn.s_quantizer_sigma': 41.617304093369626, 'model.layers.25.mlp.gate_proj': 0.0, 'model.layers.25.mlp.gate_proj_mean': array(-0.00423, dtype=float16), 'model.layers.25.mlp.gate_proj_sigma': array(2.734, dtype=float16), 'model.layers.25.mlp.up_proj': 0.0, 'model.layers.25.mlp.up_proj_mean': array(-0.00423, dtype=float16), 'model.layers.25.mlp.up_proj_sigma': array(2.734, dtype=float16), 'model.layers.25.mlp.down_proj': 0.0, 'model.layers.25.mlp.down_proj_mean': array(-0.002565, dtype=float16), 'model.layers.25.mlp.down_proj_sigma': array(2.477, dtype=float16)}, {'model.layers.26.self_attn.q_proj': 0.0, 'model.layers.26.self_attn.q_proj_mean': array(-0.002632, dtype=float16), 'model.layers.26.self_attn.q_proj_sigma': array(2.66, dtype=float16), 'model.layers.26.self_attn.k_proj': 0.0, 'model.layers.26.self_attn.k_proj_mean': array(-0.002632, dtype=float16), 'model.layers.26.self_attn.k_proj_sigma': array(2.66, dtype=float16), 'model.layers.26.self_attn.v_proj': 0.0, 'model.layers.26.self_attn.v_proj_mean': array(-0.002632, dtype=float16), 'model.layers.26.self_attn.v_proj_sigma': array(2.66, dtype=float16), 'model.layers.26.self_attn.o_proj': 0.0, 'model.layers.26.self_attn.o_proj_mean': array(-0.02057, dtype=float16), 'model.layers.26.self_attn.o_proj_sigma': array(1.43, dtype=float16), 'model.layers.26.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.26.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 0.00011112511856481433, 'model.layers.26.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 77.87168933572714, 'model.layers.26.self_attn.s_quantizer': 0.0, 'model.layers.26.self_attn.s_quantizer_mean': -0.2234973907470703, 'model.layers.26.self_attn.s_quantizer_sigma': 40.570925550201586, 'model.layers.26.mlp.gate_proj': 0.0, 'model.layers.26.mlp.gate_proj_mean': array(-0.00647, dtype=float16), 'model.layers.26.mlp.gate_proj_sigma': array(3.084, dtype=float16), 'model.layers.26.mlp.up_proj': 0.0, 'model.layers.26.mlp.up_proj_mean': array(-0.00647, dtype=float16), 'model.layers.26.mlp.up_proj_sigma': array(3.084, dtype=float16), 'model.layers.26.mlp.down_proj': 0.0, 'model.layers.26.mlp.down_proj_mean': array(-0.0009265, dtype=float16), 'model.layers.26.mlp.down_proj_sigma': array(2.404, dtype=float16)}, {'model.layers.27.self_attn.q_proj': 0.0, 'model.layers.27.self_attn.q_proj_mean': array(-0.000371, dtype=float16), 'model.layers.27.self_attn.q_proj_sigma': array(2.709, dtype=float16), 'model.layers.27.self_attn.k_proj': 0.0, 'model.layers.27.self_attn.k_proj_mean': array(-0.000371, dtype=float16), 'model.layers.27.self_attn.k_proj_sigma': array(2.709, dtype=float16), 'model.layers.27.self_attn.v_proj': 0.0, 'model.layers.27.self_attn.v_proj_mean': array(-0.000371, dtype=float16), 'model.layers.27.self_attn.v_proj_sigma': array(2.709, dtype=float16), 'model.layers.27.self_attn.o_proj': 0.0, 'model.layers.27.self_attn.o_proj_mean': array(0.0004728, dtype=float16), 'model.layers.27.self_attn.o_proj_sigma': array(1.391, dtype=float16), 'model.layers.27.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.27.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 9.757006773725152e-05, 'model.layers.27.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 75.7099729229908, 'model.layers.27.self_attn.s_quantizer': 0.0, 'model.layers.27.self_attn.s_quantizer_mean': 0.06027406454086304, 'model.layers.27.self_attn.s_quantizer_sigma': 38.49675310984031, 'model.layers.27.mlp.gate_proj': 0.0, 'model.layers.27.mlp.gate_proj_mean': array(-0.00929, dtype=float16), 'model.layers.27.mlp.gate_proj_sigma': array(3.121, dtype=float16), 'model.layers.27.mlp.up_proj': 0.0, 'model.layers.27.mlp.up_proj_mean': array(-0.00929, dtype=float16), 'model.layers.27.mlp.up_proj_sigma': array(3.121, dtype=float16), 'model.layers.27.mlp.down_proj': 0.0, 'model.layers.27.mlp.down_proj_mean': array(-0.01276, dtype=float16), 'model.layers.27.mlp.down_proj_sigma': array(2.97, dtype=float16)}, {'model.layers.28.self_attn.q_proj': 0.0, 'model.layers.28.self_attn.q_proj_mean': array(-0.000696, dtype=float16), 'model.layers.28.self_attn.q_proj_sigma': array(2.79, dtype=float16), 'model.layers.28.self_attn.k_proj': 0.0, 'model.layers.28.self_attn.k_proj_mean': array(-0.000696, dtype=float16), 'model.layers.28.self_attn.k_proj_sigma': array(2.79, dtype=float16), 'model.layers.28.self_attn.v_proj': 0.0, 'model.layers.28.self_attn.v_proj_mean': array(-0.000696, dtype=float16), 'model.layers.28.self_attn.v_proj_sigma': array(2.79, dtype=float16), 'model.layers.28.self_attn.o_proj': 0.0, 'model.layers.28.self_attn.o_proj_mean': array(-0.00838, dtype=float16), 'model.layers.28.self_attn.o_proj_sigma': array(1.51, dtype=float16), 'model.layers.28.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.28.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -7.615960203111172e-05, 'model.layers.28.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 70.8519583356734, 'model.layers.28.self_attn.s_quantizer': 0.0, 'model.layers.28.self_attn.s_quantizer_mean': 0.08281227946281433, 'model.layers.28.self_attn.s_quantizer_sigma': 36.61966684720111, 'model.layers.28.mlp.gate_proj': 0.0, 'model.layers.28.mlp.gate_proj_mean': array(0.001917, dtype=float16), 'model.layers.28.mlp.gate_proj_sigma': array(2.79, dtype=float16), 'model.layers.28.mlp.up_proj': 0.0, 'model.layers.28.mlp.up_proj_mean': array(0.001917, dtype=float16), 'model.layers.28.mlp.up_proj_sigma': array(2.79, dtype=float16), 'model.layers.28.mlp.down_proj': 0.0, 'model.layers.28.mlp.down_proj_mean': array(-0.005142, dtype=float16), 'model.layers.28.mlp.down_proj_sigma': array(2.467, dtype=float16)}, {'model.layers.29.self_attn.q_proj': 0.0, 'model.layers.29.self_attn.q_proj_mean': array(0.009346, dtype=float16), 'model.layers.29.self_attn.q_proj_sigma': array(2.758, dtype=float16), 'model.layers.29.self_attn.k_proj': 0.0, 'model.layers.29.self_attn.k_proj_mean': array(0.009346, dtype=float16), 'model.layers.29.self_attn.k_proj_sigma': array(2.758, dtype=float16), 'model.layers.29.self_attn.v_proj': 0.0, 'model.layers.29.self_attn.v_proj_mean': array(0.009346, dtype=float16), 'model.layers.29.self_attn.v_proj_sigma': array(2.758, dtype=float16), 'model.layers.29.self_attn.o_proj': 0.0, 'model.layers.29.self_attn.o_proj_mean': array(-0.02853, dtype=float16), 'model.layers.29.self_attn.o_proj_sigma': array(1.976, dtype=float16), 'model.layers.29.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.29.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 9.396171662956476e-05, 'model.layers.29.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 77.14920608794364, 'model.layers.29.self_attn.s_quantizer': 0.0, 'model.layers.29.self_attn.s_quantizer_mean': -0.1597442626953125, 'model.layers.29.self_attn.s_quantizer_sigma': 40.55859958134649, 'model.layers.29.mlp.gate_proj': 0.0, 'model.layers.29.mlp.gate_proj_mean': array(0.003876, dtype=float16), 'model.layers.29.mlp.gate_proj_sigma': array(2.828, dtype=float16), 'model.layers.29.mlp.up_proj': 0.0, 'model.layers.29.mlp.up_proj_mean': array(0.003876, dtype=float16), 'model.layers.29.mlp.up_proj_sigma': array(2.828, dtype=float16), 'model.layers.29.mlp.down_proj': 0.0, 'model.layers.29.mlp.down_proj_mean': array(-0.000232, dtype=float16), 'model.layers.29.mlp.down_proj_sigma': array(2.457, dtype=float16)}, {'model.layers.30.self_attn.q_proj': 0.0, 'model.layers.30.self_attn.q_proj_mean': array(-0.0001659, dtype=float16), 'model.layers.30.self_attn.q_proj_sigma': array(2.76, dtype=float16), 'model.layers.30.self_attn.k_proj': 0.0, 'model.layers.30.self_attn.k_proj_mean': array(-0.0001659, dtype=float16), 'model.layers.30.self_attn.k_proj_sigma': array(2.76, dtype=float16), 'model.layers.30.self_attn.v_proj': 0.0, 'model.layers.30.self_attn.v_proj_mean': array(-0.0001659, dtype=float16), 'model.layers.30.self_attn.v_proj_sigma': array(2.76, dtype=float16), 'model.layers.30.self_attn.o_proj': 0.0, 'model.layers.30.self_attn.o_proj_mean': array(0.04413, dtype=float16), 'model.layers.30.self_attn.o_proj_sigma': array(1.8, dtype=float16), 'model.layers.30.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.30.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': -6.177695468068123e-05, 'model.layers.30.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 73.32121111929344, 'model.layers.30.self_attn.s_quantizer': 0.0, 'model.layers.30.self_attn.s_quantizer_mean': 0.7345578670501709, 'model.layers.30.self_attn.s_quantizer_sigma': 39.698866482558415, 'model.layers.30.mlp.gate_proj': 0.0, 'model.layers.30.mlp.gate_proj_mean': array(-0.00878, dtype=float16), 'model.layers.30.mlp.gate_proj_sigma': array(2.883, dtype=float16), 'model.layers.30.mlp.up_proj': 0.0, 'model.layers.30.mlp.up_proj_mean': array(-0.00878, dtype=float16), 'model.layers.30.mlp.up_proj_sigma': array(2.883, dtype=float16), 'model.layers.30.mlp.down_proj': 0.0, 'model.layers.30.mlp.down_proj_mean': array(-0.00593, dtype=float16), 'model.layers.30.mlp.down_proj_sigma': array(2.92, dtype=float16)}, {'model.layers.31.self_attn.q_proj': 0.0, 'model.layers.31.self_attn.q_proj_mean': array(0.005672, dtype=float16), 'model.layers.31.self_attn.q_proj_sigma': array(2.395, dtype=float16), 'model.layers.31.self_attn.k_proj': 0.0, 'model.layers.31.self_attn.k_proj_mean': array(0.005672, dtype=float16), 'model.layers.31.self_attn.k_proj_sigma': array(2.395, dtype=float16), 'model.layers.31.self_attn.v_proj': 0.0, 'model.layers.31.self_attn.v_proj_mean': array(0.005672, dtype=float16), 'model.layers.31.self_attn.v_proj_sigma': array(2.395, dtype=float16), 'model.layers.31.self_attn.o_proj': 0.0, 'model.layers.31.self_attn.o_proj_mean': array(-0.00853, dtype=float16), 'model.layers.31.self_attn.o_proj_sigma': array(1.061, dtype=float16), 'model.layers.31.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': 0.0, 'model.layers.31.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': 8.966683526523411e-05, 'model.layers.31.self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': 72.27724399837061, 'model.layers.31.self_attn.s_quantizer': 0.0, 'model.layers.31.self_attn.s_quantizer_mean': -0.10453426837921143, 'model.layers.31.self_attn.s_quantizer_sigma': 48.641546028061235, 'model.layers.31.mlp.gate_proj': 0.0, 'model.layers.31.mlp.gate_proj_mean': array(-0.00508, dtype=float16), 'model.layers.31.mlp.gate_proj_sigma': array(2.676, dtype=float16), 'model.layers.31.mlp.up_proj': 0.0, 'model.layers.31.mlp.up_proj_mean': array(-0.00508, dtype=float16), 'model.layers.31.mlp.up_proj_sigma': array(2.676, dtype=float16), 'model.layers.31.mlp.down_proj': 0.0, 'model.layers.31.mlp.down_proj_mean': array(-0.003754, dtype=float16), 'model.layers.31.mlp.down_proj_sigma': array(2.12, dtype=float16)}]
Component-wise statistics across all 32 layers:
Component				Min_K		Max_P
--------------------------------------------------------------------------------
mlp.down_proj                           	0.000000e+00
mlp.down_proj_mean                      	5.157471e-03
mlp.down_proj_sigma                     	2.970703e+00
mlp.gate_proj                           	0.000000e+00
mlp.gate_proj_mean                      	1.378632e-02
mlp.gate_proj_sigma                     	3.121094e+00
mlp.up_proj                             	0.000000e+00
mlp.up_proj_mean                        	1.378632e-02
mlp.up_proj_sigma                       	3.121094e+00
self_attn.apply_rotary_pos_emb_qk_rotation_wrapper	0.000000e+00
self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean	2.744787e-04
self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma	7.927168e+01
self_attn.k_proj                        	0.000000e+00
self_attn.k_proj_mean                   	1.818848e-02
self_attn.k_proj_sigma                  	2.789062e+00
self_attn.o_proj                        	0.000000e+00
self_attn.o_proj_mean                   	4.412842e-02
self_attn.o_proj_sigma                  	2.294922e+00
self_attn.q_proj                        	0.000000e+00
self_attn.q_proj_mean                   	1.818848e-02
self_attn.q_proj_sigma                  	2.789062e+00
self_attn.s_quantizer                   	0.000000e+00
self_attn.s_quantizer_mean              	7.345579e-01
self_attn.s_quantizer_sigma             	6.471476e+01
self_attn.v_proj                        	0.000000e+00
self_attn.v_proj_mean                   	1.818848e-02
self_attn.v_proj_sigma                  	2.789062e+00
Global maximum p across all components: 79.27168473042566
{'global_max_p': 79.27168473042566, 'component_stats': {'mlp.down_proj': {'max_p': 0.0}, 'mlp.down_proj_mean': {'max_p': array(0.005157, dtype=float16)}, 'mlp.down_proj_sigma': {'max_p': array(2.97, dtype=float16)}, 'mlp.gate_proj': {'max_p': 0.0}, 'mlp.gate_proj_mean': {'max_p': array(0.01379, dtype=float16)}, 'mlp.gate_proj_sigma': {'max_p': array(3.121, dtype=float16)}, 'mlp.up_proj': {'max_p': 0.0}, 'mlp.up_proj_mean': {'max_p': array(0.01379, dtype=float16)}, 'mlp.up_proj_sigma': {'max_p': array(3.121, dtype=float16)}, 'self_attn.apply_rotary_pos_emb_qk_rotation_wrapper': {'max_p': 0.0}, 'self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_mean': {'max_p': 0.00027447869069874287}, 'self_attn.apply_rotary_pos_emb_qk_rotation_wrapper_sigma': {'max_p': 79.27168473042566}, 'self_attn.k_proj': {'max_p': 0.0}, 'self_attn.k_proj_mean': {'max_p': array(0.01819, dtype=float16)}, 'self_attn.k_proj_sigma': {'max_p': array(2.79, dtype=float16)}, 'self_attn.o_proj': {'max_p': 0.0}, 'self_attn.o_proj_mean': {'max_p': array(0.04413, dtype=float16)}, 'self_attn.o_proj_sigma': {'max_p': array(2.295, dtype=float16)}, 'self_attn.q_proj': {'max_p': 0.0}, 'self_attn.q_proj_mean': {'max_p': array(0.01819, dtype=float16)}, 'self_attn.q_proj_sigma': {'max_p': array(2.79, dtype=float16)}, 'self_attn.s_quantizer': {'max_p': 0.0}, 'self_attn.s_quantizer_mean': {'max_p': 0.7345578670501709}, 'self_attn.s_quantizer_sigma': {'max_p': 64.71475874945375}, 'self_attn.v_proj': {'max_p': 0.0}, 'self_attn.v_proj_mean': {'max_p': array(0.01819, dtype=float16)}, 'self_attn.v_proj_sigma': {'max_p': array(2.79, dtype=float16)}}}
